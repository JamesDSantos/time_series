[
  {
    "objectID": "garch.html#o-modelo-de-heterocedasticidade-condicional-autorregressiva---arch",
    "href": "garch.html#o-modelo-de-heterocedasticidade-condicional-autorregressiva---arch",
    "title": "14  Modelos heterocedásticos",
    "section": "14.1 O modelo de heterocedasticidade condicional autorregressiva - ARCH",
    "text": "14.1 O modelo de heterocedasticidade condicional autorregressiva - ARCH\nSeja \\(\\sigma^2_{t|t-1}\\) a variância (ou volatidade) condicional de \\(x_t\\) com \\(x_{t-1}\\). O modelo ARCH(1) é definido por\n\\[\\begin{align}x_t&=\\sigma_{t|t-1}\\varepsilon_t\\\\\n\\sigma^2_{t|t-1}&=\\omega+\\alpha x_{t-1}^2,\\end{align}\\]\nonde \\(\\omega\\) e \\(\\alpha\\) são parâmetros, \\(\\{\\varepsilon_t\\}\\) é uma sequência de variáveis indepedentes e identicamente distribuídas com média zero e variância um, com \\(\\varepsilon_t\\) independente de \\(x_{t-j}\\) para \\(j=1,2,\\ldots\\). Como \\[\\begin{align}E(x_t^2|\\mathcal{D}_{t-1})&=E(\\sigma^2_{t|t-1}\\varepsilon_t^2|\\mathcal{D}_{t-1})\\\\&=\\sigma^2_{t|t-1}E(\\varepsilon_t^2|\\mathcal{D}_{t-1})=\\sigma^2_{t|t-1}Var(\\varepsilon_t)=\\sigma^2_{t|t-1}\\end{align}\\] teremos que \\(x_t^2\\) é estimador para \\(\\sigma^2_{t|t-1}\\).\nNote que \\[Cov(x_t,x_{t-h})=Cov(\\varepsilon_t(\\omega+\\alpha x_{t-1}^2),\\varepsilon_{t-h}(\\omega+\\alpha x_{t-h-1}^2))=0,\\] logo, o correlograma não releva qualquer informação sobre esse processo. Contudo, seja \\(\\eta_t = x_t^2- \\sigma^2_{t|t-1}\\). É imediato qu \\(E(\\eta_t)=0\\) e \\[Var(\\eta_t)=E([\\sigma_{t|t-1}^2\\varepsilon_t^2-\\sigma^2_{t|t-1}]^2)\\]. Agora, observe que, para $h $, \\[\\begin{align}Cov(\\eta_t,\\eta_{t-h})&=Cov(x_{t}^2-\\sigma^2_{t|t-1}, x_{t-h}^2-\\sigma^2_{t-h|t-h-1})\\\\&=Cov(x_t^2-\\omega-\\alpha x_{t-1}^2,x_{t-h}^2 - \\omega -\\alpha x_{t-h-1}^2)=0,\\end{align}\\] Logo, \\(\\{\\eta_t\\}\\) é uma sequência com média zero de variáveis não correlacionadas. Assumindo que \\(x_t\\) é estacionário, \\[Var(\\eta_t)=Var(x_t^2 -\\alpha x_{t-1}^2)=Var(x^2)(1+\\alpha^2).\\] temos que \\(\\eta_t\\) é estacionário. Como \\[\\sigma^2_{t|t-1}=\\omega + \\alpha x_{t-1}^2\\Rightarrow x_t^2=\\omega+\\alpha x_{t-1}^2+\\eta_t,\\] se \\(x_t\\) é um ARCH(1), então \\(x_t^2\\) é um processo AR(1). Assumindo que \\(x_t^2\\) é um processo estacionário, teremos que\n\\[E(x_t^2)=\\omega + \\alpha E(x_{t-1}^2)\\Rightarrow \\sigma^2=\\omega + \\alpha\\sigma^2,\\] logo, a variância estacionária é dada por \\[\\sigma^2=\\frac{\\omega}{1-\\alpha},\\] o que implica que \\(\\alpha\\in[0,1)\\).\nO principal uso do modelo ARCH é para a previsão das variâncias condicionais futuras, dada por \\[\\sigma_t^2(h)=E(x_{t+h}^2|\\mathcal{D}_t).\\] Para \\(h=1\\) no modelo ARCH(1) teremos\n\\[\\sigma_t^2(1)=\\omega+\\alpha x_t^2=(1-\\alpha)\\sigma^2+\\alpha x_t^2,\\] que é a média ponderada entre a estimativa \\(x_t\\) para \\(\\sigma^2_{t|t-1}\\) e a variância estacionária. A previsão para \\(h&gt;0\\) qualquer no modelo ARCH(1) é dada por\n\\[\\begin{align}\\sigma^2_t(h)&=E(x_{t+h}^2|\\mathcal{D}_t)=E(\\sigma^2_{t+h|t+h-1}\\varepsilon_{t+h}^2|\\mathcal{D}_t)\\\\ &=E\\left(E(\\sigma^2_{t+h|t+h-1}\\varepsilon_{t+h}^2|\\mathcal{D}_{t+h-1})|\\mathcal{D}_t\\right)\\\\\n&=E\\left(E(\\sigma^2_{t+h|t+h-1}|\\mathcal{D}_{t+h-1})E(\\varepsilon_{t+h}^2)|\\mathcal{D}_t\\right)\\\\&=\nE\\left(\\omega+\\alpha x_{t+h-1}^2|\\mathcal{D}_t\\right)=\\omega+\\alpha E\\left( x_{t+h-1}^2|\\mathcal{D}_t\\right)\\\\\n&=\n\\omega +\\alpha\\sigma^2_t(h-1)\\end{align}\\]\nO modelo ARCH(\\(q\\)) é dado por \\[\\begin{align}x_t&=\\sigma_{t|t-1}\\varepsilon_t\\\\\\sigma_{t|t-1}&=\\omega + \\sum_{j=1}^q \\alpha_j x_{t-j}^2.\\end{align}\\]"
  },
  {
    "objectID": "garch.html#o-modelo-de-heterocedasticidade-condicional-autorregressiva-generalizado-garch",
    "href": "garch.html#o-modelo-de-heterocedasticidade-condicional-autorregressiva-generalizado-garch",
    "title": "14  Modelos heterocedásticos",
    "section": "14.2 O modelo de heterocedasticidade condicional autorregressiva generalizado (GARCH)",
    "text": "14.2 O modelo de heterocedasticidade condicional autorregressiva generalizado (GARCH)\nA generalização do modelo ARCH adiciona variâncias condicionais defasadas, gerando no modelo\n\\[\\begin{align}x_t&=\\sigma_{t|t-1}\\varepsilon_t\\\\\\sigma_{t|t-1}^2&=\\omega + \\sum_{j=1}^q \\alpha_j x_{t-j}^2+ \\sum_{j=1}^p \\beta_j\\sigma^2_{t|t-j}.\\end{align}\\] Esse modelo é denotado por GARCH(\\(p,q\\)) (embora exista na literatura a notação GARCH(\\(q,p\\)), logo sempre verifique a documentação do programa que você está utilizando para sabera ordem correta de \\(p\\) e \\(q\\)). É usual se referir à \\(p\\) como a ordem do GARCHe à \\(q\\) como a ordem do ARCH.\nPara identificar a ordem do modelo GARCH, considere novamente a transformação \\(\\eta_t=x_t^2 - \\sigma^2_{t|t-1}\\). Então,\n\\[\\begin{align}\\sigma_{t|t-1}^2&=\\omega + \\sum_{j=1}^q \\alpha_j x_{t-j}^2+ \\sum_{j=1}^p \\beta_j\\sigma^2_{t|t-j}\\\\\n&=\\omega + \\sum_{j=1}^q \\alpha_j x_{t-j}^2+ \\sum_{j=1}^p \\beta_j(\\sigma^2_{t|t-j}+\\eta_{t-j})-\\sum_{j=1}^p\\beta_j\\eta_{t-j}\\\\&=\\omega + \\sum_{j=1}^q \\alpha_j x_{t-j}^2+ \\sum_{j=1}^p \\beta_jx^2_{t-j}-\\sum_{j=1}^p\\beta_j\\eta_{t-j}.\\end{align}\\] Considere que \\(\\alpha_k=0\\) para \\(k&gt;q\\) e \\(\\beta_k=0\\) para \\(k&gt;p\\). Então, a equação acima se reduz à \\[\\begin{align}\\sigma_{t|t-1}^2&=\\omega + \\sum_{j=1}^{\\max\\{p,q\\}} (\\alpha_j +\\beta_j)x_{t-j}^2 -\\sum_{j=1}^p\\beta_j\\eta_{t-j}.\\end{align}\\] e, somando \\(\\eta_t\\) em ambos os lados da equação acima, teremos \\[\\begin{align}x_t^2&=\\omega + \\sum_{j=1}^{\\max\\{p,q\\}} (\\alpha_j +\\beta_j)x_{t-j}^2 +\\eta_t-\\sum_{j=1}^p\\beta_j\\eta_{t-j},\\end{align}\\] logo \\(x_t^2\\) é um ARMA(\\(\\max\\{p,q\\},p\\)) com deriva. Desse modo, a autocorrelação parcial amostral de \\(x_t^2\\) deve trazer informações sobre \\(\\max\\{p,q\\}\\), enquanto que o correlograma deve fazer o mesmo com \\(p\\). Se \\(\\max\\{p,q\\}=p\\), veremos posteriormente como obter evidência sobre o valor de \\(q\\).\nSuponha que o modelo GARCH, de \\(\\{x_t\\}\\), é estacionário. Então, aplicando a esperança na equação anterior, teremos\n\\[\\begin{align}\\sigma^2&=\\omega + \\sigma^2\\sum_{j=1}^{\\max\\{p,q\\}} (\\alpha_j +\\beta_j)\\end{align}\\] logo, a variância estacionária será \\[\\sigma^2=\\frac{\\omega}{1-\\sum_{j=1}^{\\max\\{p,q\\}}(\\alpha_j+\\beta_j)}\\] que é finita se \\(\\sum_{j=1}^{\\max\\{p,q\\}}(\\alpha_j+\\beta_j)&lt;1,\\) sendo essa uma condição necessária e suficiente para que \\(\\{x_t\\}\\) seja estacionária.\nA previsão é dada por \\[\\begin{align}\\sigma_t^2(h)&=E\\left(\\omega + \\sum_{j=1}^{\\max\\{p,q\\}} (\\alpha_j +\\beta_j)x_{t+h-j}^2 +\\eta_t-\\sum_{j=1}^p\\beta_j\\eta_{t+h-j}|\\mathcal{D}_t\\right)\\\\&=\\omega + \\sum_{j=1}^{\\max\\{p,q\\}} (\\alpha_j +\\beta_j)E\\left(x_{t+h-j}^2|\\mathcal{D}_t\\right)\\\\&=\\omega + \\sum_{j=1}^{\\max\\{p,q\\}} (\\alpha_j +\\beta_j)\\sigma_{t}^2(h-j)E\\left(x_{t+h-j}^2|\\mathcal{D}_t\\right)\\\\&=\\omega + \\sum_{j=1}^{\\max\\{p,q\\}} (\\alpha_j +\\beta_j)\\sigma^2_t(t+h-j)\\end{align}\\] onde \\(\\sigma^2_{t}(h-j)=x_{t+h}^2\\) se \\(h\\leq j\\).\nVamos construir a função de verossimilhança do modelo GARCH(1,1) (o modelo geral é análogo). Lembremos que\n\\[\\begin{align}x_t&=\\sigma_{t|t-1}\\varepsilon_t\\\\ \\sigma^2_{t|t-1}&=\\omega+\\alpha x_{t-1}^2+\\beta\\sigma^2_{t-1|t-2},\\end{align}\\] Fixando o valor inicial \\(\\sigma^2_{1|0}\\), teremos que \\(x_1\\sim\\hbox{Normal}(0, \\sigma^2_{1|0})\\) e, a partir dessa condição, podemos calcular \\[\\sigma^2_{2|1}=\\omega+\\alpha x_1^2 + \\beta \\sigma^2_{1|0},\\] logo \\(x_2|x_1\\sim\\hbox{Normal}(0, \\sigma^2_{2|1})\\). É simples induzir que \\[x_t|\\mathcal{D}_{t-1}\\sim\\hbox{Normal}(0,\\sigma^2_{t|t-1})\\] e podemos escrever a função densidade conjunta de \\(x_1,\\ldots,x_n\\) como \\[f(x_1,\\ldots,x_n)=f(x_n|\\mathcal{D}_{n-1})f(x_1,\\ldots,x_{n-1}),\\] e, a partir disso, temos a função de verossimilhança para \\(\\theta=(\\omega, \\sigma_{1|0}^2, \\alpha,\\beta)\\). Alguns autores fazem \\[\\sigma^2_{1|0}=\\sigma^2=\\frac{\\omega}{1-\\alpha-\\beta}.\\] As estimativas de máxima verossimilhança são obtidas via maximização numérica.\nOs resíduos desse modelo são dados por \\[\\hat{\\varepsilon}_t=\\frac{x_t}{\\hat{\\sigma}_{t|t-1}}\\] e, se o modelo estiver adequado, \\(\\{\\hat{\\varepsilon}_t\\}\\) é uma sequência de variáveis independentes e identicamente distribuídas. Infelizmente, o poder o teste de Box-Pierce é baixo contra variáveis não correlacionadas mas dependentes, como é o caso do modelo GARCH. A alternativa é utilizar o teste de McLeod-Li, uma generalização do teste de Box-Pierce. Em geral, ele é apresentado como um gráfico para cada defasagem, onde o modelo ARCH é rejeitado se todos os p-valores são maiores que 5%. As defasagens com p-valores menores que 5% dão evidências sobre a ordem \\(q\\) do modelo."
  },
  {
    "objectID": "garch.html#exemplo-sp500",
    "href": "garch.html#exemplo-sp500",
    "title": "14  Modelos heterocedásticos",
    "section": "14.3 Exemplo: S&P500",
    "text": "14.3 Exemplo: S&P500\nSeja \\(p_t\\) o preço de ação ou um índice de preço. O retorno de uma aplição feita no tempo \\(t\\) após \\(h\\) unidades de tempo é definido por \\[\\frac{p_{t+h}}{p_t}.\\] Um mercado saudável possui um retorno entre o tempo \\(t\\) e \\(t-1\\) aproximadamente igual a 1. Vamos definir o log-retorno como \\[x_t=\\log(p_t)-\\log(p_{t-1}).\\] Desse modo, é esperado que \\(x_t\\approx 0\\). A variância desse tipo de retorno costuma apresentar volatidade. Nesse caso, a variância condicional é uma importante medida de risco (em finanças, ela recebe o nome de Var - Value at Risk) e, quando maior, mais arriscado é o invertimento.\nO Standard and Poor’s 500 (S&P 500) é um índice do mercado de ações que reúne as 500 maiores empresas do mundo listadas na NYSE e na Nasdaq, principais Bolsas de Valores dos Estados Unidos. Abaixo, mostramos o log-retorno diário dessa série.\n\nlibrary(gsheet)\n\nWarning: package 'gsheet' was built under R version 4.3.2\n\nurl &lt;- 'https://docs.google.com/spreadsheets/d/1fZU9H-ENlO2kSNYAjFX3pAE8OjA0m1cqfaiqTgSaX7Q/edit?usp=sharing'\nbova11 &lt;- gsheet2tbl(url)\nn &lt;- length(bova11$Último)\nretorno &lt;- diff( log(bova11$Último[-(201:274)] ))\nts.plot(retorno)\n\n\n\n\nAbaixo, vemos o correlograma da série e de \\(x^2_t\\). Perceba que a primeira mostra que a série é não correlacionada, mas o segundo mostra que os valores de \\(\\{x_t\\}\\) não são independentes (se fossem, não haveria autocorrelações significativas para \\(x_t^2\\)). O correlograma dá indícios de que \\(p=1\\), enquanto que a autocorrelação parcial dá indícios de que \\(\\max\\{p,q\\}=3\\). Portanto, um modelo candidato seria GARCH(1,3).\n\nacf(retorno)\n\n\n\nacf(retorno^2)\n\n\n\npacf(retorno^2)\n\n\n\n\nPodemos estimar os parâmetros do modelo GARCH através da função garch, no pacote tseries.\n\nrequire(tseries)\n\nCarregando pacotes exigidos: tseries\n\n\nWarning: package 'tseries' was built under R version 4.3.3\n\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\nmod0 &lt;- garch( retorno, order = c(1,3), trace =F)\ncoefficients(mod0)\n\n          a0           a1           a2           a3           b1 \n1.686619e-04 1.237287e-01 1.795927e-02 2.119220e-01 3.500818e-16 \n\n\nAcima, \\(a_0=\\hat{\\omega}\\), \\(a_i=\\hat{\\alpha}_i\\) e \\(b_i=\\hat{\\beta}_i\\).\nAbaixo, o gráfico dos resíduos mostra que a volatilidade no final da série diminuiu.\n\nres0 &lt;- residuals(mod0)[-c(1:3)]\nts.plot(res0)\n\n\n\n\nO correlograma e o gráfico da função de autocorrelação parcial dos resíduos ao quadrado não apresenta autocorrelação significativa. Já o teste de McLeod-Li rejeita a hipótese de uma estrutura ARCH. Por último, o teste de Shapiro-Wilks não rejeita a normalidade.\n\nrequire(TSA)\n\nCarregando pacotes exigidos: TSA\n\n\nWarning: package 'TSA' was built under R version 4.3.2\n\n\n\nAttaching package: 'TSA'\n\n\nThe following objects are masked from 'package:stats':\n\n    acf, arima\n\n\nThe following object is masked from 'package:utils':\n\n    tar\n\nacf(res0^2)\n\n\n\npacf(res0^2)\n\n\n\nMcLeod.Li.test(y = res0)\n\n\n\nshapiro.test(res0)\n\n\n    Shapiro-Wilk normality test\n\ndata:  res0\nW = 0.98636, p-value = 0.0556"
  },
  {
    "objectID": "regressao.html#as-funções-ts.union-e-ts.intersect",
    "href": "regressao.html#as-funções-ts.union-e-ts.intersect",
    "title": "12  Modelos de regressão para séries temporais",
    "section": "12.1 As funções ts.union e ts.intersect",
    "text": "12.1 As funções ts.union e ts.intersect\nConsidere novamente a série de mortes por doenças pulmonares no Reino Unido. Além da série ldeaths, o pacote datasets possui as séries mdeahts e fdeaths, que representam as mortes para os sexos masculino e feminino, respectivamente. Assim, como a primeira, essas duas séries são mensais, começando em janeiro de 1974 e terminando em dezembro de 1979.\nDesde de dois objetos ts tenham o mesmo argumento frequency, é possível organizá-las lado a lado e um data.frame respeitando o tempo do registro de cada observação. Para tanto, utilizamos a função ts.union. Ilustramos isso com aas duas novas séries apresentadas.\n\nd1 &lt;- ts.union( ldeaths, mdeaths)\nhead(d1)\n\n     ldeaths mdeaths\n[1,]    3035    2134\n[2,]    2552    1863\n[3,]    2704    1877\n[4,]    2554    1877\n[5,]    2014    1492\n[6,]    1655    1249\n\n\nObserve que é possível realizar a organização mesmo que os tempos de início ou fim sejam diferentes - nesse caso, NAs serão gerados quando não houverem registros. Veja o exemplo abaixo, no qual o primeiro mês da série mdeaths foi removido\n\nmdeaths2 &lt;- window(mdeaths, start = c(1974,2))\nd2 &lt;- ts.union( ldeaths, mdeaths2)\nhead(d2)\n\n     ldeaths mdeaths2\n[1,]    3035       NA\n[2,]    2552     1863\n[3,]    2704     1877\n[4,]    2554     1877\n[5,]    2014     1492\n[6,]    1655     1249\n\n\nDe modo análogo, podemos construir data.frame que apresenta um subconjunto das séries corresponde à interseção dos períodos registrados, utilizando a função ts.intersect\n\nmdeaths3 &lt;- window(mdeaths, start = c(1974,2), end = c(1974,5))\nd3 &lt;- ts.intersect( ldeaths, mdeaths3)\nd3\n\n         ldeaths mdeaths3\nFeb 1974    2552     1863\nMar 1974    2704     1877\nApr 1974    2554     1877\nMay 1974    2014     1492"
  },
  {
    "objectID": "regressao.html#a-função-de-correlação-cruzada-e-a-correlação-espúria",
    "href": "regressao.html#a-função-de-correlação-cruzada-e-a-correlação-espúria",
    "title": "12  Modelos de regressão para séries temporais",
    "section": "12.2 A função de correlação cruzada e a correlação espúria",
    "text": "12.2 A função de correlação cruzada e a correlação espúria\nSejam \\(\\{x_t\\}\\) \\(\\{y_t\\}\\) duas séries temporais. A função de covariância cruzada é dada por\n\\[\\gamma_{r,s}(X,Y)=Cov(X_r,Y_s).\\] Se as séries forem estacionárias, a covariância cruzada é função somente da diferença \\(k=r-s\\) e escrevemos \\[\\gamma_{k}(X,Y)=Cov(X_{t+k},Y_t)=Cov(X_{t},Y_{t-k}).\\]\nObserve que a função de covariância cruzada é ímpar, uma vez que\n\\[\\gamma_{k}(X,Y)\\neq \\gamma_{-k}(X,Y).\\]\nAlém disso, a função de autocovariância, de \\(X\\) por exemplo, é dada por \\(\\gamma_k(X,X)\\).\nA função de correlação cruzada é definida por \\[\\rho_k(X,Y)=\\frac{\\gamma_{k}(X,Y)}{\\sqrt{Var(x)Var(y)}}\\] :::{#exm-} Considere que \\[y_t = \\beta_0+\\beta_1 x_{t-d}+\\varepsilon_t,\\] onde \\(\\varepsilon_t\\) é um ruído branco com variância \\(\\nu\\) e \\(x_t\\) tem distribuição normal com variância \\(\\sigma^2_X\\) e são independentes entre si e do ruído. Como,\n\\[\\begin{align}\nVar(y_t)=\\beta_1^2Var(x_{t-d})+Var(\\varepsilon_t)=\\beta_1^2\\sigma^2_X+\\nu.\n\\end{align}\\] e \\[\\begin{align}\nCov(x_t,y_{t-k})&=Cov(x_t,\\beta_0+\\beta_1x_{t-k-d}+\\varepsilon_{t-k})\\\\&=\\beta_1Cov(x_t,x_{t-k-d})=\\left\\{\\begin{array}{ll}\\beta_1\\sigma_X^2,&\\hbox{se }k=-d,\\\\0,&\\hbox{caso contrário}\\end{array}\\right.\n\\end{align}\\] logo, \\[\\rho_k(X,Y)=\\left\\{\\begin{array}{ll}\\frac{\\beta_1\\sigma_X}{\\sqrt{\\beta_1^2\\sigma_X^2+\\nu}},&\\hbox{ se }k=-d,\\\\0,&\\hbox{caso contrário}\\end{array}\\right.\\] \\(\\blacksquare\\) :::\nPara duas séries \\(x_t\\) e \\(y_t\\) observadas, de tamanho \\(n\\), a função de correlação cruzada amostral (CCF) é definida por \\[r_k(X,Y)=\\frac{\\sum_{t=k+1}^n (x_t-\\bar{x}_n)(y_{t-k}-\\bar{y}_n)}{\\sqrt{\\sum_{t=1}^n(x_t-\\bar{x}_n)^2}\\sqrt{\\sum_{t=1}^n(y_t-\\bar{y}_n)^2}}\\] e a regra\npode ser utilizada para detectar se Para o modelo \\[y_t=\\beta_0+\\beta_1 x_{t-d}+\\varepsilon_t,\\] \\(x\\) é independente de \\(y\\) se e somente se \\(\\beta_1=0\\). Nesse caso, \\(r_k(X,Y)\\) tem distribuição aproximadamente normal com média zero e variância \\(1/n\\) e a regra \\[|r_k(X,Y)|&gt;\\frac{1,96}{\\sqrt{n}}\\] pode ser utilizada para detectar se a defasagem \\(k\\) na regressora é significativa. Entretanto, esse modelo é muito restritivo e um modelo mais geral seria\n\\[y_t= \\beta_0+\\beta_1 x_{t-d}+z_t,\\] onde \\(z_t\\) é um processo ARIMA. Nesse caso, mesmo se \\(x_t\\) e \\(y_t\\) forem processos estacionários, a variância de \\(r_k(X,Y)\\) não é mais aproximadamente \\(1/n\\), sendo geralmente inflada. Isso implica que, ao utilizar a regra de comparar o módulo de \\(r_k(X,Y)\\) com \\(1,96/\\sqrt{n}\\), é possível encontrar correlações que não fazem sentido, ou seja, espúrias.\n:::{#exm-} Como exemplo de correlação espúria, considere a produção mensal de leite e a produção de energia elétrica (em escala logaritmica) nos EUA entre janeiro de 1994 e dezembro de 2006. Note como hà várias correlações significativas, quando comparadas com o limiar 1,96/\\(\\sqrt{n}\\). Entretanto, não parece fazer sentido a relação entre produção de eletricidade e de leite.\n\nrequire(TSA)\n\nCarregando pacotes exigidos: TSA\n\n\nWarning: package 'TSA' was built under R version 4.3.2\n\n\n\nAttaching package: 'TSA'\n\n\nThe following objects are masked from 'package:stats':\n\n    acf, arima\n\n\nThe following object is masked from 'package:utils':\n\n    tar\n\ndata(milk); data(electricity)\nleiteLogEle &lt;- ts.intersect(milk, log(electricity))\nplot(leiteLogEle)\n\n\n\nccf( leiteLogEle[,1] , leiteLogEle[,2] )"
  },
  {
    "objectID": "regressao.html#whitening",
    "href": "regressao.html#whitening",
    "title": "12  Modelos de regressão para séries temporais",
    "section": "12.3 Whitening",
    "text": "12.3 Whitening\nConsidere o modelo\n\\[y_t= \\sum_{j=-\\infty}^\\infty \\beta_j x_{t-j}+z_t,\\] onde \\(z_t\\) é um processo ARIMA. Se ambos \\(\\{x_t\\}\\) e \\(\\{y_t\\}\\) são estacionários, a variância de \\(r_k(X,Y)\\) é aproximadamente \\[\\frac{1}{n}\\left[1+2\\sum_{k=1}^\\infty \\rho_k(x)\\rho_k(y)\\right]\\] onde \\(\\rho_k(x)\\) e \\(\\rho_k(y)\\) são as funções de autocorrelação de \\(\\{x_t\\}\\) e \\(\\{y_t\\}\\), respectivamente. Note que, se pelo menos um dos processos for um ruído branco, então a variância será aproximadamente 1/\\(n\\).\nSuponha então \\(\\{x_t\\}\\) é um processo ARIMA, \\[\\phi(B)(1-B)^d x_t=\\theta(B)\\epsilon_t,\\] onde \\(\\epsilon_t\\) é ruído branco. Suponha ainda que o processo é inversível, ou seja, existe \\(\\pi(B)\\) tal que \\[\\pi(B)x_t=\\epsilon_t.\\] Então, \\(\\pi(B)x_t\\) é um ruído branco. A aplicação do filtro \\(\\pi(B)\\) em \\(x_t\\) para obter um ruído branco é denominado whitening ou prewhitening. Aplicando o filtro ao modelo no começo dessa seção, teremos\n\\[\\pi(B)y_t= \\sum_{j=-\\infty}^\\infty \\pi(\\beta)x_{t-j}+\\pi(B)\\varepsilon_t.\\]\nComo \\(\\pi(B)x_t\\) é independente de \\(\\pi(B)\\varepsilon_t\\), a correlação cruzara entre \\(\\pi(B)y_t\\) e \\(\\pi(B)x_t\\) é igual à \\[\\rho_k(\\pi(X),\\pi(Y))=\\beta_{-k}\\sqrt{\\frac{Var(\\pi(B)x_t)}{Var(\\pi(B)y_t)}},\\] ou seja, a correlação cruzada do processo whitened é proporcional à \\(\\beta_{-k}\\) e, sob \\(H_0\\), pode-se verificar a significância da defasagem comparando o módulo da correlação cruzada amostral com 1,96\\(/\\sqrt{n}\\).\n\nExample 12.1 Considere novamente as séries de produção de leite e eletricidade vistas na seção anterior. Ambas possuem tendência e sazonalidade, que podem ser removidas com operadores diferença para obter um processo estacionário.\n\nestacionario &lt;- ts.intersect(diff(diff(milk,12)),\ndiff(diff(log(electricity),12)))\n\nAbaixo, encontramos a correlação cruzada utilizando o processo whitened. Note que as correlações espúrias se foram e não há correlação digna de nota.\n\nprewhiten( estacionario[,1],  estacionario[,2])"
  },
  {
    "objectID": "regressao.html#regressão-para-séries-temporais",
    "href": "regressao.html#regressão-para-séries-temporais",
    "title": "12  Modelos de regressão para séries temporais",
    "section": "12.4 Regressão para séries temporais",
    "text": "12.4 Regressão para séries temporais\nSejam \\(\\{x_t\\}\\) e \\(\\{y_t\\}\\) séries temporais onde\n\\[y_t=\\sum_{j=-\\infty}^\\infty \\beta_j x_{t-j}+z_t,\\] onde \\(z_t\\) é um modelo ARIMA. Na prática, apenas um número finito dos \\(\\beta\\)’s é diferente de zero. Seja \\(\\dot{x}_t=\\pi(B)x_t\\), onde \\(\\dot{x}_t\\) é um processo AR(\\(\\infty\\)). Seja \\(\\dot{y}_t=\\pi(B)\\). Então,\n\\[\\dot{y}_t=\\sum_{j=-\\infty}^\\infty \\beta_j \\dot{x}_{t-j}+\\dot{z}_t,\\] onde \\(\\dot{z}_t=\\pi(B)z_t\\). Como \\(\\dot{x}_t\\) é um ruído branco e independente de \\(\\dot{z}_t\\), a correlação cruzada entre \\(\\dot{x}_t\\) e \\(\\dot{y}_t\\) na defasagem \\(k\\) é igual à \\(\\beta_{-k}\\sigma_{\\dot{X}}/\\sigma_{\\dot{Y}}.\\)\nAbaixo, segue um roteiro para uma análise preliminar: 1. Se necessário, remova o sinal de \\(x_t\\) de \\(y_t\\), aplicando os operadores diferença necessários. 2. Encontre uma representação de um modelo AR para \\(x_t\\), minimizando o AIC por exemplo, estime \\(\\pi(B)\\) 3. Obtenha \\(\\dot{x}_t=\\pi(B)x_t\\) e \\(\\dot{y}_t=\\pi(B)y_t.\\) 4. Estime a função de correlação cruzada entre \\(\\dot{y}_t\\) e \\(\\dot{x}_t\\). Determine \\(\\mathcal{K}\\), um conjunto de defasagens significativas 5. Encontre os estimadores de mínimos quadrados para \\(\\beta\\), minimizando\n\\[\\sum_{t=1}^n z_t^2=\\sum_{t=1}^n\\left(y_t-\\sum_{j\\in\\mathcal{K}}\\beta_j x_{t-j}\\right)^2\\] 6. Estime \\(z_t\\) pelos resíduos\n\\[\\hat{z}_t=y_t-\\sum_{j\\in\\mathcal{K}}\\hat{\\beta}_j x_{t-j}\\] e determine o modelo ARIMA estimado para \\(z_t\\) 7. Encontre os estimadores de máxima verossimilhança para o modelo\n\\[y_t=\\sum_{j\\in\\mathcal{K}} \\beta_j x_{t-j}+z_t\\] onde \\(z_t\\) é um modelo ARIMA com a ordem encontrada no passo anterior.\n\nVamos simular um processo do seguinte modelo:\n\\[y_t = 2x_{t-4}+z_t,\\] onde \\(x_t\\) é um processo AR(1) com \\(\\phi=.8\\) e \\(z_t\\) um ARMA(1,1) com \\(\\phi=.2\\) e \\(\\theta=.5\\).\n\nset.seed(123)\nz &lt;- arima.sim( list( ar = .2, ma = .5), 105)\n\nx &lt;- arima.sim( list(ar = .8, ma = 0), 105)\n\nx &lt;- ts(x, start = 1); z &lt;- ts(z, start=1)\n\ny &lt;- NULL\nfor(t in 5:105) y[t] &lt;- 2*x[t-4] + z[t]\n\nx &lt;- ts(x[5:105])\ny &lt;- ts(y[-(1:4)])\n\nplot.ts( cbind(y,x))\n\n\n\n\nObserve abaixo que a correlação cruzada apresenta a defasagem -4 significativa e conunto com várias outras, que são espúrias.\n\nccf(x,y)\n\n\n\n\nAs séries \\(x_t\\) e \\(y_t\\) já parecem ser estacionárias. Abaixo, aplicamos o whitening e as correlações espúrias desapareceram. Temos evidências de que apenas \\(\\beta_{4}\\) é relevante.\n\nrequire(TSA)\nprewhiten(x,y)\n\n\n\n\nVamos ajustar o modelo de regressão \\[y_t=\\beta_{4}x_{t-j}\\] e obter o AIC e seus resíduos.\n\nmod &lt;- lm( y[5:100] ~ x[1:96] -1)\nAIC(mod)\n\n[1] 296.0433\n\nresReg &lt;- residuals(mod)\n\nVamos analisar os resíduos obtibos para determinar o modelo de \\(z_t\\). Abaixo, vemos indícios para o modelo ARMA(1,1).\n\nacf(resReg)\n\n\n\npacf(resReg)\n\n\n\n\nVamos ajustar o modelo de regressão para séries temporais:\n\nmodFinal &lt;- arima(y[5:100] , order = c(1,0,1), xreg = x[1:96] , include.mean =  FALSE)\nmodFinal$aic\n\n[1] 256.901\n\n\n\\(\\blacksquare\\)"
  }
]