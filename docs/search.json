[
  {
    "objectID": "processo_linear_geral.html#o-polinômio-de-defasagens",
    "href": "processo_linear_geral.html#o-polinômio-de-defasagens",
    "title": "10  Modelos ARMA",
    "section": "10.1 O polinômio de defasagens",
    "text": "10.1 O polinômio de defasagens\n\nDefinition 10.1 Considere uma sequência \\(\\{a_t\\}\\). O operador defasagem é definido como \\(Ba_t = a_{t-1}\\). Em inglês este operador é conhecido como backshift.\n\n\nExample 10.1 Considere a série \\(11,6,3,7,15,8\\).\n\\[\\begin{align*}\n     Ba_2 &= a_{1}=11\\\\\n     B a_6& =a_5 =15\n    \\end{align*}\\] \\(\\blacksquare\\)\n\nSeguem algumas propriedades fundamentais do operador defasagem\n\n\\(Bc=c\\)\n\\(B^k a_t=a_{t-k}\\).\n\\(B(c+ba_{t-1})=c+bBa_t\\) (operador linear)\n\\((B^m - B^n)a_t = a_{t-m}-a_{t-n}.\\)\n\\(B^{-1}a_t = a_{t+1}\\)\n\nNote que o operador \\(B^{-1}\\) leva \\(a_t\\) para um passo a frente (como em uma previsão). É comum encontrar a definição \\(F=B^{-1}\\), onde a letra \\(F\\) é escolhida por causa do termo (previsão).\n\nExample 10.2 Seja \\(B\\) o operado defasagem. Então, definimos um polinômio de defasagens como \\[\\psi(B)=a_0+a_1B+\\cdots+a_n B^n.\\]\n\nNote que \\(\\psi(B)\\) é um modo sucinto para escrever \\[a(B)x_t=a_0x_t+a_1x_{t-1}+\\cdots+a_n x_{t-n}.\\]\nO polinômio de defasagens pode ser operado como um polinômio regular. Por exemplo, se \\(a(B)=1-aB\\) e \\(b(B)=1-bB\\), fazer \\[\\begin{align}\na(B)b(B)x_t&=a(B)[(1-bB)x_t]=a(B)(x_t-bx_{t-1})\\\\\n&=a(B)x_t-ba(B)x_{t-1}=(1-aB)x_t-b(1-aB)x_{t-1}\\\\\n&=x_t-(a+b)x_{t-1}+abx_{t-2}\\end{align}\\] é equivalente a encontrar \\[c(B)=(1-aB)(1-bB)=1-(a+b)B + abB^2\\] e calcular \\[c(B)x_t=x_t-(a+b)x_{t-1}+abx_{t-2}.\\]\nDizemos que o polinômio de defasagens \\(\\psi(B)\\) possui inversa se existe uma função \\(\\psi^{-1}(B)\\) tal que \\(\\psi(B)\\psi^{-1}(B)x_t=x_t\\).\n\nExample 10.3 Para entender corretamente os modelos que serão propostos, é fundamental entender o que é a inversa do polinômio de defasagens. O caso mais importante, que será a chave para os demais, é o baseado no seguinte polinômio: \\[\\psi(B)=1-\\phi B.\\]\nSuponha que \\[y_t=(1-\\phi B)x_t=x_t-\\phi x_{t-1 }\\] Estamos procurando que qual situação existe \\(\\psi^{-1}(B)\\) tal que \\[x_t=\\psi^{-1}(B)y_t.\\] Como \\[y_{t-1}=(1-\\phi B)x_{t-1}=x_{t-1}-\\phi x_{t-2},\\] teremos que \\[y_t=x_t-\\phi[y_{t-1}-\\phi x_{t-2}]=x_t-\\phi y_{t-1}-\\phi^2x_{t-2}.\\] Note que podemos continuar iterando as equações acima, obtendo \\[y_t=x_t-\\sum_{j=1}^{t-1} \\phi^jy_{t-j}-\\phi^{t}x_0.\\] Assuma que \\(|\\phi|&lt;1\\) e que \\(t\\) é grande, o que implica que \\(\\phi^t x_0\\) é despresível. Então \\[y_t=x_t-\\sum_{j=1}^{t-1}\\phi^j y_{t-j}=x_t-\\sum_{j=1}^{t-1}\\phi^j B^j y_t\\] ou ainda \\[\\left(1-\\sum_{j=1}^{t-1}\\phi^j B^j\\right)y_t=x_t\\] Agora, multiplique os dois lados da equação acima por \\(1-\\phi B\\). Então \\[(1-\\phi B)\\left(1-\\sum_{j=1}^{t-1}\\phi^j B^j\\right)y_t=(1-\\phi B)x_t=y_t \\] Deste modo, \\[\\psi^{-1}(B)=\\lim_{t\\rightarrow \\infty }\\left(1-\\sum_{j=1}^{t}\\phi^j B^j\\right)=1-\\sum_{j=1}^{\\infty}\\phi^j B^j\\]\n\\(\\blacksquare\\)\n\nNo exemplo anterior, o fato de \\(|\\phi|&lt;1\\) garante que a inveresa do polinômio de defasagens existem uma vez que \\(\\lim_{t\\rightarrow\\infty}\\sum_{j=1}^t \\phi^jB^j\\) é convergente.\nO resultado geral é baseado no seguinte teorema.\n\nProposition 10.1 Seja \\(T\\) uma matriz quadrada qualquer e seja \\[S_n=\\sum_{j=1}^n T^j.\\] A série \\(S_n\\) converge quando \\(n\\rightarrow\\infty\\) se e somente se todos os autovalores de \\(T\\) são menores que um em módulo. Nesse caso, \\(T^j\\rightarrow \\textbf{0}\\) quando \\(j\\rightarrow\\infty\\) e \\[S_n\\rightarrow (1-T)^{-1}.\\]\n\nA proposição acima é a chave para demonstrar o seguite teorema\n\nTheorem 10.1 Seja \\[y_t=x_t-\\sum_{j=1}^p\\phi_jx_{t-j}=\\left(1-\\sum_{j=1}^p\\phi_jB^j\\right)x_t=\\phi(B).\\] Então, existe \\(\\phi^{-1}(B)\\) se e somente se o módulo das raízes de \\(\\phi(B)\\) são maiores que um.\n\n\nProof. Faremos a demonstração para o caso \\(p=2\\), mas os mesmos passos podem ser seguidos para demonstrar o caso geral.\nSeja \\[y_t=x_t-\\phi_1x_{t-1}-\\phi_2 x_{t-2}=(1-\\phi_1B-\\phi_2 B^2)x_t.\\] Comecemos notando que\n\\[\\left(\\begin{array}{c} x_t \\\\ x_{t-1}\\end{array}\\right)=\\left(\\begin{array}{c} y_t \\\\ 0\\end{array}\\right)+\\left(\\begin{array}{cc} \\phi_1 & \\phi_2 \\\\ 1 & 0\\end{array}\\right)\\left(\\begin{array}{c} x_{t-1} \\\\ x_{t-2}\\end{array}\\right)\\] Fazendo \\(z_t=(x_t\\;\\;x_{t-1})\\), teremos \\[z_t=\\left(\\begin{array}{c} y_t \\\\ 0\\end{array}\\right)+\\underbrace{\\left(\\begin{array}{cc} \\phi_1 & \\phi_2 \\\\ 1 & 0\\end{array}\\right)}_{A}z_{t-1}.\\] Utilizando essa relação recursiva, teremos \\[z_t=A^t z_{0}+\\sum_{j=0}^{t-1}A^j\\left(\\begin{array}{c} y_{t-j} \\\\ 0\\end{array}\\right)\\] e, notando que \\(x_t=(1\\;\\;0)z_t\\), teremos \\[x_t=(1\\;\\;0)A^t z_{0}+(1\\;\\;0)\\sum_{j=0}^{t-1}A^j\\left(\\begin{array}{c} y_{t-j} \\\\ 0\\end{array}\\right)\\] Suponha que os autovalores de \\(A\\) são, em módulo, maiores que um. Então, pela Proposition 10.1, para \\(t\\) suficientemente grande, \\[\\begin{align}x_t&=(1\\;\\;0)\\sum_{j=1}^\\infty A^j \\left(\\begin{array}{c} y_{t-j} \\\\ 0\\end{array}\\right)=(1\\;\\;0)\\sum_{j=1}^\\infty A^j B^j\\left(\\begin{array}{c} y_{t} \\\\ 0\\end{array}\\right)\\\\&=\\sum_{j=1}^\\infty (1\\;\\;0)A^j B^j\\left(\\begin{array}{c} 1 \\\\ 0\\end{array}\\right)y_{t}=\\phi^{-1}(B)y_t\\end{align}\\] Agora, observe que os autovalores de \\(A\\) são obtidos através da solução de\n\\[\\begin{align}0=&\\left|\\left(\\begin{array}{cc}\\phi_1 & \\phi_2 \\\\ 1 & 0\\end{array}\\right)-\\lambda \\textbf{I}\\right|-\\lambda(\\phi_1-\\lambda)-\\phi_2\\\\&=\\lambda^2-\\lambda \\phi_1-\\phi_2\\\\\n&=1-\\frac{1}{\\lambda}\\phi_1-\\phi_2\\frac{1}{\\lambda^2}\\end{align}\\] Fazendo \\(\\lambda = 1/B\\), teremos que a equação acima se torna \\[0=1-\\frac{1}{\\lambda}\\phi_1-\\phi_2\\frac{1}{\\lambda^2}\\equiv 1-\\phi_1 B-\\phi_2B^2=\\phi(B)\\] logo, se o módulo das raízes de \\(\\phi(B)\\) são maiores que um, então o módulo dos autovalores de \\(A\\) são menores que um e, portanto, existe \\(\\phi^{-1}(B)\\)."
  },
  {
    "objectID": "processo_linear_geral.html#o-modelo-autorregressivo",
    "href": "processo_linear_geral.html#o-modelo-autorregressivo",
    "title": "10  Modelos ARMA",
    "section": "10.2 O modelo autorregressivo",
    "text": "10.2 O modelo autorregressivo\nO modelo autorregressivo de ordem \\(p\\), ou \\(AR(p)\\), é dado por \\[\\begin{equation}\n        y_t = \\sum_{i=1}^{p}\\phi_iy_{t-i} +\\varepsilon_t\n\\end{equation}\\] onde \\(\\{\\varepsilon_t\\}\\) é um ruído branco, tipicamente Normal\\((0,\\nu)\\). Nesse modelo, a contribuição da observação \\(y_{t-j}\\) em \\(y_t\\) é dada por \\(\\phi_j\\), que é invariante no tempo.\nUtilizando o polinômio de defasagens, pode-se escrever o modelo AR(\\(p\\)) como \\[\\begin{equation}\n        \\phi(B)y_t = \\varepsilon_t,\n\\end{equation}\\] onde \\(\\phi(B)=1-B\\phi_1-\\cdots \\phi_p B^p\\). Se o módulo das raízes desse polinômio são maiores que um, então existe \\(\\phi^{-1}(B)\\), ou seja \\[y_t=\\phi^{-1}(B)\\varepsilon_t=\\left(\\sum_{j=1}^\\infty \\psi_jB^j\\right)\\varepsilon_t\\] e o processo será estacionário, com média e variância iguais à \\[\\begin{align}E(y_t)&=\\left(\\sum_{j=1}^\\infty \\psi_jB^j\\right)E(\\varepsilon_t)=0\\\\\nVar(y_t)&=Var\\left(\\sum_{j=1}^\\infty \\psi_jB^j\\varepsilon_t\\right)=\\nu\\sum_{j=1}^\\infty \\psi_j^2\\\\\n\\end{align}\\] e a função de auto covariância é dada por \\[\\begin{align}\n\\gamma(h)&=Cov(y_t,y_{t-h})\\\\&=Cov\\left( \\left(\\sum_{j=1}^\\infty \\psi_jB^j\\right)\\varepsilon_t,\\left(\\sum_{k=1}^\\infty \\psi_kB^k\\right)\\varepsilon_{t-h}\\right)\\\\\n&=\\sum_{j=1}^\\infty \\sum_{k=1}^\\infty \\psi_j\\psi_k Cov\\left(   \\varepsilon_{t-j},\\varepsilon_{t-k-h}\\right)\\\\\n&=\\nu\\sum_{j=1}^\\infty \\psi_j\\psi_{j+h}\\end{align} \\tag{10.1}\\]\n\nExample 10.4 Considere o processo \\(AR(1)\\) abaixo: \\[\\begin{align*}\n    x_t(1-B\\phi)&= \\varepsilon_{t-1}.\n\\end{align*}\\]\nSeja \\(\\dot{B}\\) a raiz do polinômio \\(1-B\\phi\\). Temos que \\[\\begin{align*}\n    \\phi(\\dot{B})=0\\Rightarrow 1-\\phi \\dot{B} =0 \\Rightarrow \\dot{B} =\\frac{1}{\\phi}\n\\end{align*}\\]\nLogo, o processo AR(1) é estacionário se \\[|\\dot{B}|&gt;1\\Rightarrow \\left|\\frac{1}{\\phi}\\right|&gt;1\\Rightarrow |\\phi|&lt; 1\\] Nesse caso, já vimos que \\(\\phi^{-1}(B)=1-\\sum_{j=1}^\\infty\\phi^j B^j\\). Então, pela equação (Equation 10.1), identificando \\(\\psi_j=\\phi^j\\),teremos que\n\\[\\gamma(h)=\\nu\\sum_{j=1}^\\infty \\phi^j\\phi^{j+h}=\\nu\\phi^h\\frac{\\phi^2}{1-\\phi^2} \\] Assim, a função de autocorrelação é dada por \\[\\rho(h)=\\frac{\\gamma(h)}{\\gamma(0)}=\\phi^h.\\]\nNote que:\n\nSe \\(\\phi\\in(0,1)\\), então \\(\\rho(h)\\) decai exponencialmente para zero\nSe \\(\\in(-1,0)\\), então \\(\\rho(h)\\) decai exponencialmente para zero, mas alternando o sinal.\n\n\n\n\n\n\n\\(\\blacksquare\\)\n\n\nExample 10.5 Considere que \\(x_t\\) é um AR(\\(2\\)), ou seja, \\[x_t=\\phi_1 x_{t-1}+\\phi_2 x_{t-2}+\\varepsilon_t=\\phi(B)\\varepsilon_t,\\] onde \\(\\phi(B)=1-\\phi_1B-\\phi_2 B^2\\). As raízes desse polinômio são \\[\\dot{B}=\\frac{1}{2\\phi_2}\\left[\\phi_1\\pm\\sqrt{\\phi_1^2+4\\phi_2}\\right]\\] e, considerando que \\(|\\dot{B}|&gt;1\\), o processo será estacionário se \\((\\phi_1,\\phi_2)\\) pertence ao triângulo delimitado pelos vértices (-2,-1), (0,1), (2,-1). É interessante notarmos que, se \\(\\phi_2&lt;0\\), é possível que as raízes de \\(\\phi(B)\\) sejam um par de números complexos conjugados.\n\\(\\blacksquare\\)\n\n\n10.2.1 Função de autocorrelação para o \\(AR(p)\\)\nConsideremos o processo \\[x_t = \\sum_{j=1}^p\\phi_j x_{t-j}+\\varepsilon_t.\\] e suponha que ele é estacionário. Multiplicando ambos os lados por \\(x_{t-h}\\) e aplicando a esperança, teremos \\[\\begin{align}\nE(x_{t-h}x_t)&=\\sum_{j=1}^p \\phi_j E(x_{t-h}x_{\nt-j})\n\\end{align}\\] e, como \\(\\gamma(h)=E(x_t x_{t-h})\\), teremos \\[\\begin{align}\n\\gamma(h)&=\\sum_{j=1}^p \\phi_j \\gamma(h-j)\n\\end{align} \\tag{10.2}\\] Dividindo ambos os lados por \\(\\gamma(0)\\), teremos \\[\\begin{align}\n\\rho(h)&=\\sum_{j=1}^p \\phi_j \\rho(h-j)\n\\end{align}\\] Essa relação pode ser utilizada para encontrar a função de autocorrelação do processo sem a necessidade de encontrar a inversa de \\(\\phi(B)\\).\n\nExample 10.6 Considere o processo \\(AR(2)\\) abaixo: \\[x_{t}=\\phi_1y_{t-1}+\\phi_2y_{t-2}+\\varepsilon_t.\\] Multiplicando a equação acima por \\(x_{t-1}\\) em ambos os lados teremos \\[x_{t}x_{t-1}=\\phi_1x_{t-1}^2+\\phi_2x_{t-2}x_{t-1}+\\varepsilon_t x_{t-1}.\\] Calculando o valor esperado, temos \\[\\begin{align*}\n    \\gamma(1)&=\\phi_1E(x_{t-1}^2)+\\phi_2E(x_{t-2}x_{t-1})+E(\\varepsilon_t x_{t-1})\\\\\n    &=\\phi_1\\gamma(0) + \\phi_2\\gamma(1).\n    \\end{align*}\\] Dividindo ambos os lados por \\(\\gamma(0)\\) teremos \\[\\rho(1)=\\phi_1+\\phi_2\\rho(1),\\] logo, \\[\\rho(1)=\\frac{\\phi_1}{1-\\phi_2}\n\\]\nDe modo análogo, teremos \\[x_{t-2} x_{t}=x_{t-2}\\left(\\phi_1x_{t-1}+\\phi_2x_{t-2}+\\varepsilon_{t}\\right).\\] Aplicando a esperança, teremos \\[\\begin{align*}\n    \\gamma(2)&=\\phi_1 \\gamma(1)+\\phi_2\\gamma(0).\n    \\end{align*}\\] e dividindo os dois lados por \\(\\gamma(0)\\) teremos\n\\[\\rho(2)=\\phi_1\\rho(1)+\\phi_2=\\frac{\\phi_1^2}{1-\\phi_2}+\\phi_2\\] Com os valores de \\(\\rho(1)\\) e \\(\\rho(2)\\), podemos encontrar \\(\\rho(3)\\):\n\\[\\rho(3)=\\phi_1\\rho(2)+\\phi_2\\rho(1)\\] e assim sucessivamente.\n\\(\\blacksquare\\)\n\nEm geral, a função de autocorrelação do processo AR(\\(p\\)) pode ser escrita como\n\\[\\rho(h)=\\sum_{j=1}^p c_j\\left(\\frac{1}{\\dot{B}_j}\\right)^h\\] onde \\(\\dot{B}\\), novamente, é a raiz de \\(\\phi(B)=1-\\sum_{j=1}^p \\phi_j B^j\\).\nO comportamento da função de autocorrelação depende das raízes de \\(\\phi(B)\\).\n\nAs raízes reais se comportam de acordo com o que já foi visto no \\(AR(1)\\), decaindo exponencialmente.\nAs raízes complexas tem um comportamento de onda abafada exponencialmente.\n\n\nExample 10.7 Vamos mostrar esse resultado para o caso \\(p=2\\) (o caso geral é análogo). Teremos que\n\\[\\begin{align}\n\\left(\\begin{array}{c}\\rho(h)\\\\ \\rho(h-1)\\end{array}\\right)=\\underbrace{\\left(\\begin{array}{cc}\\phi_1 & \\phi_2 \\\\ 1 & 0 \\end{array}\\right)}_{T}\\left(\\begin{array}{c}\\rho(h-1)\\\\ \\rho(h-2)\\end{array}\\right)=\\cdots T^h\\left(\\begin{array}{c}\\rho(0)\\\\ \\rho(-1)\\end{array}\\right)\\end{align}\\]\nJá discutimos, na primeira seção, que os autovalores de \\(T\\) são equivalentes aos recíprocos das raizes de \\(\\phi(B)\\). Agora, considere a decomposição de \\(T\\) na forma canônica \\[T=Q\\Lambda Q^{-1},\\] onde \\(Q\\) é a matriz cujas colunas são formadas pelos autovetores de \\(T\\) e \\(\\Lambda\\) é uma matriz diagonal formada pelos autovalores. Note que\n\\[A^2=Q\\Lambda Q^{-1}Q\\Lambda Q^{-1}=Q\\Lambda^2 Q^{-1}\\] e, de modo análogo, \\[A^h=Q\\Lambda Q^{-1}Q\\Lambda Q^{-1}=Q\\Lambda^h Q^{-1}\\] Como \\(\\Lambda^h\\) é a matriz diagonal com os autovalores elevados à potência \\(h\\), os elementos de \\(A^h\\) devem ser necessariamente do tipo \\(\\sum_{j=1}^2 c_j \\lambda_j^h\\).\nO polinômio característico da matriz \\(T\\) é \\[\\lambda^2-\\phi_1\\lambda-\\phi_2\\] e, denominando suas raizes por \\(\\lambda_1\\) e \\(\\lambda_2\\), teremos \\[\\rho(h)=c_1\\lambda_1^h+c_2\\lambda_2^h.\\] - Se \\(\\lambda_1\\) é real então \\(\\lambda_2\\) também é real e o decaimento de \\(\\rho(h)\\) será exponencial, podendo alternar o sinal se algum dos autovalores for negativo.\n\nSe \\(\\lambda_1\\) é complexo, então \\(\\lambda_2\\) será seu conjugado. Em coordenadas polares, teremos \\(\\lambda_1=re^{i\\omega}\\) e \\(\\lambda_2=re^{-i\\omega}\\). Deste modo, \\[\\begin{align}\\rho(h)&=r^h [c_1e^{ih\\omega}+c_2e^{-ih\\omega}]\\\\&=r^h[(c_1+c_2)\\cos(h\\omega)+i(c_1-c_2)\\sin(h\\omega)]\\end{align}\\] logo, \\(\\rho(h)\\) terá um comportamento de onda com a amplitude decrescendo exponencialmente. A próxima figura mostra esse efeito.\n\n\n\n\n\n\n\\(\\blacksquare\\)\n\n\n\n10.2.2 A função de autocorrelação parcial (PACF)\nInicialmente, considere a regressão \\[\\hat{x}_t=\\sum_{j=1}^p \\beta_j x_{j-j}.\\]\nonde \\(\\beta\\) é o valor de minimiza \\[E(x_t-\\hat{x}_t)^2=E\\left(x_t-\\sum_{j=1}^p \\beta_jx_{t-j}\\right)^2. \\tag{10.3}\\] Para calcular \\(\\phi_{hh}\\) devemos minimizar \\[\\begin{align}E(x_{t+h}-\\hat{x}_{t+h})^2&=\\gamma(0)+\\sum_{j=1}^p\\sum_{k=1}^p\\beta_j\\beta_k \\gamma(j-k)-2\\sum_{j=1}^p\\beta_j\\gamma(j)\\end{align}\\] e, encontrando o ponto crítico após derivar em \\(\\beta\\), concluímos que o valor de \\(\\beta\\) que minimiza (Equation 10.3) satisfaz. \\[\\gamma(j)=\\sum_{k=1}^p \\beta_k\\gamma(j-k).\\] Contudo, comparando a equação acima com a identidade dada na Equation 10.2, temos que \\(\\beta_k=\\phi_k,\\) ou seja,\n\\[\\hat{x}_t=\\sum_{j=1}^p\\phi_jx_{t-j}.\\] É interessante notarmos que chegaremos ao mesmo resultado de definirmos \\[\\hat{x}_t=\\sum_{j=1}^p \\phi_j x_{t+j}.\\] Vamos alternar entre essas duas formas de acordo com a conveniência.\n\nDefinition 10.2 A função de autocorrelação parcial, denotada por \\(\\phi_{hh}\\) é \\[\\phi_{11}=\\hbox{corr}(x_{t+1},x_t),\\] e, para \\(h\\geq 2\\), \\[\\phi_{hh}=\\hbox{corr}(x_{t+h}-\\hat{x}_{t+h},x_t-\\hat{x}_t\n).\\] Além disso, se \\(x_t\\) é um processo gaussiano, então \\[\\phi_{hh}=\\hbox{corr}(x_{t+h},x_t|x_{t+1},\\ldots,x_{t+h-1}).\\] \\(\\blacksquare\\)\n\nIntuitivamente, a função de autocorrelação parcial calcula a correlação entre \\(x_t\\) e \\(x_{t+h}\\) eliminando a dependência linear entre os valores intermediários \\(x_s\\), \\(t&lt;s&lt;t+h\\).\n\nExample 10.8 Considere o processo AR(1) estacionário. Como \\(\\gamma(h)=\\phi^2\\), é imediato, pela definição, que \\(\\phi_{11}=\\gamma(1)/\\gamma(0)=\\phi\\).\nPara \\(h\\geq 2\\), vamos definir \\(\\hat{x}_{t+h}=\\phi x_{t+h-1}\\) e \\(\\hat{x}_t=\\phi x_{t+1}\\). Então, \\[\\begin{align}\\phi_{hh}&=Cov( x_{t+h}-\\hat{x}_{t+h},x_t-\\hat{x}_t)\\\\&=Cov( x_{t+h}-\\phi x_{t+h-1},x_t-\\phi x_{t+1})\\\\&=\n\\gamma(h)-\\phi\\gamma(h-1)-\\phi\\gamma(h-1)+\\phi^2\\gamma(h-2)\\\\\n\\end{align}\\] Pela Equation 10.2, \\[\\begin{align}\\gamma(h)&=\\phi \\gamma(h-1)\\\\ \\gamma(h-1)&=\\phi\\gamma(h-2) \\end{align}\\] logo, \\(\\phi_{hh}=0\\) para todo \\(h\\geq 2\\)\n\\(\\blacksquare\\)\n\n\nExample 10.9 Considere o processo AR(2) estacionário. Para \\(h=2\\), defina \\[\\begin{align}\\hat{x}_{t+2}&=\\phi_1 x_{t+1}+\\phi_2 x_{t}\\\\ \\hat{x}_t&=\\phi_1 x_{t+1}+\\phi_2 x_{t+2}\\end{align}\\] Então \\[\\begin{align}\\phi_{22}&=Cov(x_{t+2}-\\hat{x}_{t+2}, x_t-\\hat{x}_t)\\\\&=Cov(x_{t+2}-\\phi_1 x_{t+1}-\\phi_2 x_{t},x_t-\\phi_1 x_{t+1}-\\phi_2 x_{t+2})\\\\\n&=Cov(\\varepsilon_{t+2},x_t-\\phi_1 x_{t+1} - \\phi_2 x_{t+2})\\\\&=-\\phi_2Cov(\\varepsilon_{t+2},x_{t+2})\\end{align}\\]\nPor último, para qualquer \\(h&gt;2\\), \\[\\begin{align}\\phi_{hh}&=Cov(x_{t+h}-\\hat{x}_{t+h}, x_t-\\hat{x}_t)\\\\&=Cov(x_{t+h}-\\phi_1 x_{t+h-1}-\\phi_2 x_{t+h-2},x_t-\\phi_1 x_{t+1}-\\phi_2 x_{t+2})\\\\\n&=Cov(\\varepsilon_{t+2},x_t-\\phi_1 x_{t+1} - \\phi_2 x_{t+h})\\\\&=0\\end{align}\\]\n\\(\\blacksquare\\)\n\nSeja \\(x_t\\) um processo AR(\\(p\\)) estacionário. Faça, \\[\\begin{align}\\hat{x}_{t+h}&=\\sum_{j=1}^p \\phi_j x_{t+h-j}\\\\ \\hat{x}_t&=\\sum_{j=1}^p \\phi_j x_{t+j}\\end{align}\\]\nEntão,\n\\[\\begin{align}\\phi_{hh}&=Cov(x_{t+h}-\\hat{x}_{t+h},x_t-\\hat{x}_{t})\\\\\n&=Cov\\left(x_{t+h}-\\sum_{j=1}^p\\phi_j x_{t+h-j},x_{t}-\\sum_{j=1}^p\\phi_j x_{t-j} \\right)\\\\\n&=Cov\\left(\\varepsilon_{t+h},x_{t}-\\sum_{j=1}^p\\phi_j x_{t-j} \\right)\\\\ &=\\sum_{j=1}^p \\phi_j Cov\\left(\\varepsilon_{t+h}, x_{t-j} \\right)\\\\ \\end{align}\\] logo, \\(\\phi_{hh}\\) é nulo sempre que \\(h&gt;j\\).\n\nDefinition 10.3 Gráfico da autocorrelação parcial\nO gráfico da função de autocorrelação parcial é construído a partir dos pares \\((h,\\phi_{hh})\\), para \\(h=1,2,\\ldots\\). \\(\\blakcsquare\\)\n\nNo exemplo abaixo, uma amostra de tamanho 100 foi gerada de um processo AR(2) estacionário e o gráfico da função de autocorrelação parcial gerado. Note que há apenas duas autocorrelações significativas, levantando evidências de que a ordem do modelo autoregressivo é 2.\n\nset.seed(1)\nx = c(0,0)\nfor(i in 3:100) x[i] = x[i-1] -.8*x[i-2]+rnorm(1,0,.1)\n \npacf(x, xlab = 'Defasagem', ylab = 'Autocorrelação parcial', main = '', lwd = 2)\n\n\n\n\n\n\n10.2.3 Método de estimação de Yule-Walker\nO método de estimação de Yule-Walker para estimar os parâmetros do modelo AR(\\(p\\)) consiste na aplicação do método dos momentos. Sua principal vantagem é a ausência de suposição sobre a distribuição dos erros do modelo, exigindo apenas a condição de estacionaridade.\nPara o processo \\(AR(p)\\) estacionário, sabemos que \\[\\gamma(h) = \\sum_{j=1}^p \\phi_j \\gamma( h-j).\\] Considere a equações do modelo AR(\\(p\\)): \\[x_t = \\sum_{j=1}^p \\phi_j x_{t-j}+\\varepsilon_t.\\] Podemos multiplicar ambos os lados por \\(x_t\\) (ou seja \\(x_{t-h}\\), com \\(h=0\\)). Após aplicar a esperança teremos \\[E(x_t^2) = \\sum_{j=1}^p \\phi_j E(x_{t-j}x_{t})+E(\\varepsilon_tx_{t}).\\] Aqui há um detalhe importante:\n\\[E(\\varepsilon_tx_{t})=E\\left(\\varepsilon_t\\left[\\sum_{j=1}^p \\phi_j x_{t-j}+\\varepsilon_t\\right]\\right)=E(\\varepsilon_t^2)=\\nu\\] Portanto, \\[\\begin{align}   \n    \\gamma(h)=\\left\\{\\begin{array}{ll}\n    \\sum_{j=1}^p \\phi_j \\gamma( h-j),&\\;\\;\\hbox{se }h&gt;0\\\\\n    \\sum_{j=1}^p \\phi_j \\gamma( j)+\\nu,&\\;\\;\\hbox{se }h=0 \\\\\n    \\end{array}\\right.\n    \\end{align}\\]\nO método de estimação de Yule-Walker consiste em substituir \\(\\gamma(1),\\ldots,\\gamma(p)\\) pelas autocovariâncias amostrais: \\[\\begin{align*}\n\\hat{\\gamma}(1) &= \\phi_1\\hat{\\gamma}(0) + \\phi_1\\hat{\\gamma}(-1) + \\cdots + \\phi_p\\hat{\\gamma}(1-p) \\\\\n\\hat{\\gamma}(2) &= \\phi_1\\hat{\\gamma}(1) + \\phi_1\\hat{\\gamma}(0)  + \\cdots + \\phi_p\\hat{\\gamma}(2-p) \\\\\n\\vdots &= \\vdots \\\\\n\\hat{\\gamma}(p) &= \\phi_1\\hat{\\gamma}(p-1) + \\phi_1\\hat{\\gamma}(p-2)  + \\cdots + \\phi_p\\hat{\\gamma}(0)\n\\end{align*}\\] Nota: lembre-se que \\(\\gamma(-i)=\\gamma(i)\\). Podemos escrever o sistema na forma matricial: \\[\\begin{align*}\n    \\underbrace{\\left(\n    \\begin{array}{cccc}\n\\hat{\\gamma}(0) & \\hat{\\gamma}(-1) & \\cdots & \\hat{\\gamma}(1-p) \\\\\n\\hat{\\gamma}(1) & \\hat{\\gamma}(0)  & \\cdots & \\hat{\\gamma}(2-p) \\\\\n         \\vdots & \\vdots               & \\ddots                & \\vdots\\\\\n\\hat{\\gamma}(p-1) & \\hat{\\gamma}(p-2) &  \\cdots & \\hat{\\gamma}(0)   \n    \\end{array}\\right)}_{\\hat{\\boldsymbol{\\Gamma}}}\n    \\underbrace{\\left( \\begin{array}{c} \\hat{\\phi}_1 \\\\ \\hat{\\phi}_2 \\\\ \\vdots \\\\ \\hat{\\phi}_p   \\end{array}\\right)}_{\\hat{\\boldsymbol{\\phi}}}=\n    \\underbrace{\\left( \\begin{array}{c} \\hat{\\gamma}(1) \\\\ \\hat{\\gamma}(2) \\\\ \\vdots \\\\ \\hat{\\gamma}(p)   \\end{array}\\right)}_{\\hat{\\boldsymbol{\\gamma}}}.\n    \\end{align*}\\] e os estimadores de Yule-Walker para os parâmetros autoregressivos são dados por \\[\\hat{\\boldsymbol{\\phi}}=\\hat{\\boldsymbol{\\Gamma}}^{-1}\\hat{\\boldsymbol{\\gamma}}.\\]\nVoltando para a Equação (\\(\\ref{eq:autocovarianca-AR}\\)), temos que \\[\\gamma(0)=\\sum_{j=1}^{p}\\phi_j \\gamma(j)+\\nu\\] Portanto, um estimador para \\(\\nu\\) é dado por \\[\\hat{\\nu}=  \n  \\hat{\\gamma(0)}=\\sum_{j=1}^{p}\\hat{\\phi_j}\\hat{\\gamma(j)}\\]\n\n\n10.2.4 Estimador de máxima verossimilhança\nObserve que \\[x_t|\\mathcal{D}_{t-1}\\sim x_t|x_{t-1},\\ldots,x_{t-p}\\] ou seja, \\(\\{x_t\\}\\) é uma cadeia de Markov de ordem \\(p\\). Deste modo, para \\(n&gt;p\\), \\[\\begin{align*}\nf(x_{1},\\ldots,x_n)&=f\\left(x_n|\\mathcal{D}_{n-1}\\right)f\\left(\\mathcal{D}_{n-1}\\right)=f\\left(x_T|x_{n-1},\\ldots,x_{n-p}\\right)f\\left(\\mathcal{D}_{n-1}\\right)\\\\\n        &=\\prod_{t=p+1}^{n}f\\left(x_t|x_{t-1},\\ldots,x_{t-p}\\right) f(x_{1},\\ldots,x_p).\n        \\end{align*}\\] Agora, suponha que \\(\\varepsilon_t\\sim\\hbox{Normal}(0,\\nu)\\). Então, \\[\\begin{align*}\nL(\\phi_1,\\ldots,\\phi_p,\\nu)&=\\left(\\frac{1}{2\\pi\\nu}\\right)^{\\frac{n}{2}}\\exp\\left\\{-\\frac{1}{2\\nu}\\sum_{t=p+1}^n(x_t-\\sum_{j=1}^p \\phi_jx_{t-j})^2\\right\\}\\\\&\\times f(x_{1},\\ldots,x_p|\\phi_1,\\ldots,\\phi_p,\\nu).\n        \\end{align*}\\]\nO método da maxima verossimilhança condicional consiste em ignorar o termo \\(f(x_{1},\\ldots,x_p|\\phi_1,\\ldots,\\phi_p,\\nu)\\). Com isso, o restante da função de verossimilhança pode ser escrito como um modelo linear, uma vez que\n\\[x_t=\\sum_{j=1}^p \\phi_jx_{t-j}+\\varepsilon_t=\\underbrace{(x_{t-1}\\;\\;\\cdots \\;\\;x_{t-p})}_{F_t'}\\underbrace{\\left(\\begin{array}{c}\\phi_1 \\\\ \\vdots \\\\ \\phi_p\\end{array}\\right)}_{\\beta}\\] e \\(\\hat{\\beta}\\) e \\(\\hat{\\nu}\\) podem ser obtidos pelos estimadores já discutidos na seção sobre modelos lineares.\nJá para o método da máxima verossimilhança, precisamos especificar a distribuição do termo adicional \\(f(x_1,\\ldots,x_p|\\phi_1,\\ldots,\\phi_p)\\). Desde que o processo seja estacionário, é possível mostrar que \\[x_1,\\ldots,x_p\\sim\\hbox{Normal}(\\textbf{0}_p,\\Sigma(\\phi_1,\\ldots,\\phi_p))\\] onde \\(\\Sigma(\\phi_1,\\ldots,\\phi_p))\\) é a matriz formada pelas autocovariâncias.\nA maximização da função de verossimilhança é feita numericamente e pode-se utilizar as estimativas obtidas no método da máxima verossimilhança condicional como ponto de partida para o algoritmo de maximização.\n\n\n10.2.5 Previsão\nConsidere a amostra \\(\\mathcal{D}_t\\) do modelo AR(1). Então,\n\\[\\begin{align}x_{t+h}&=\\phi x_{t+h-1}+\\varepsilon_{t+h}\\\\ &=\\phi^2x_{t+h-2}+\\phi\\varepsilon_{t+h-1}+\\varepsilon_{t+h}= \\cdots\\\\&=\\phi^h x_t+\\sum_{j=0}^{h-1}\\phi^j\\varepsilon_{t+h-j}\\end{align}\\] logo \\[E(x_{t+h}|\\mathcal{D}_t)=\\phi^h x_t\\] e \\[Var(x_{t+h}|\\mathcal{D}_t)=\\nu\\sum_{j=0}^{h-1}\\phi^{2j}=\\nu\\frac{1-\\phi^{2h}}{1-\\phi^2}\\] Note que, se o processo é estacionário, então \\(|\\phi|&lt;1\\) e a previsão converge para zero exponencialmente. Na prática, tanto a previsão quanto a variância são calculadas substituíndo \\(\\phi\\) por sua estimativa.\nPara o processo AR\\((p)\\), seja \\(z_t=(x_t,\\ldots,x_{t-p+1})\\). Então, \\[z_t=Tz_{t-1}+\\boldsymbol{\\varepsilon}_t\\] onde \\[T=\\left(\\begin{array}{ccc|c}\\phi_1&\\cdots&\\phi_{p-1} & \\phi_p \\\\ \\hline & \\textbf{I}_{p-1} & & \\textbf{0}_{p-1}\\end{array}\\right)\\] e \\[\\boldsymbol{\\varepsilon}_t=\\left(\\begin{array}{c}\\varepsilon_t \\\\ \\hline \\textbf{0}_{p-1}\\end{array}\\right)\\] Logo, dado \\(\\mathcal{D}_t\\), \\[z_{t+h}=T^h z_t+\\sum_{j=0}^{h-1}T^j\\boldsymbol{\\varepsilon}_{t+h-j}\\] Note que \\(x_{t+h}=(1|\\textbf{0}'_{p-1})z_{t+h}\\), logo \\[E(x_{t+h}|\\mathcal{D}_t)=(1|\\textbf{0}'_{p-1})T^h z_t\\] e \\[Var(x_{t+h}|\\mathcal{D}_t)=\\nu\\sum_{j=0}^{h-1}\\left(\\begin{array}{c|c}1&\\textbf{0}'_{p-1}\\end{array}\\right)T^j \\mathcal{E} T'^j\\left(\\begin{array}{c}1 \\\\ \\hline \\textbf{0}_{p-1}\\end{array}\\right) \\] onde \\(\\mathcal{E}\\) é uma matriz com o valor 1 no elemento \\(a_{11}\\) e zero nos demais.\n\n\n10.2.6 Processo autorregressivo com deriva\nAté o momento, assumimos que o processo AR\\((p)\\) é estacionário com \\(E(x_t)=0\\). Quando \\(\\mu=E(x_t)\\neq 0\\), temos um parâmetro adicional, denominado deriva e escrito como\n\\[x_t-\\mu=\\sum_{j=1}^p \\phi_j(x_{t-j}-\\mu)+\\varepsilon_t\\]\nObserve que a deriva não altera a estrutura de autocorrelação do processo, uma vez que podemos definir \\(y_t=x_t-\\mu\\), onde \\(y_t\\) é um AR(\\(p\\)) com \\(E(y_t)=0\\), o que implica em \\[\\gamma(h)=Cov(x_t, x_{t-1})=Cov(x_t-\\mu,x_{t-h}-\\mu)=Cov(y_t,y_{t-h}).\\]\nContudo, a função de previsão do processo deve ser ajusta. Defina \\(z_t=(x_t-\\mu,\\ldots,x_{t-p+1}-\\mu)\\). Já vimos que\n\\[z_{t+h}=T^h z_t+\\sum_{j=0}^{h-1}T^j\\boldsymbol{\\varepsilon}_{t+h-j}\\] onde \\(T\\) e \\(\\boldsymbol{\\varepsilon}_t\\) foram definidos na seção anterior. Note que \\(x_{t+h}-\\mu=(1|\\textbf{0}'_{p-1})z_{t+h}\\), logo \\[E(x_{t+h}|\\mathcal{D}_t)=\\mu+(1|\\textbf{0}'_{p-1})T^h z_t\\]\nEm termos de estimação, a função de verossimilhança deve ser alterada para\n\\[\\begin{align*}\nL(\\mu,\\phi_1,\\ldots,\\phi_p,\\nu)&=\\left(\\frac{1}{2\\pi\\nu}\\right)^{\\frac{n}{2}}\\exp\\left\\{-\\frac{1}{2\\nu}\\sum_{t=p+1}^n(x_t-\\mu-\\sum_{j=1}^p \\phi_j(x_{t-j}-\\mu)^2\\right\\} \\\\&\\times f(x_{1},\\ldots,x_p|\\mu,\\phi_1,\\ldots,\\phi_p,\\nu).\n        \\end{align*}\\]\nonde \\[x_{1},\\ldots,x_p\\sim\\hbox{Normal}(\\mu\\textbf{1}_p,\\Sigma(\\phi_1,\\ldots,\\phi_h))\\]\n\n\n10.2.7 Exemplo: número anual de terremotos\nA série abaixo apresenta o número anual de terremotos de magnitude maior ou igual à 7 na escala Richter.\n\nrequire(gsheet)\n\nCarregando pacotes exigidos: gsheet\n\n\nWarning: package 'gsheet' was built under R version 4.3.2\n\nurl &lt;- 'https://docs.google.com/spreadsheets/d/1PPf1nOjwh1fnr1VtFW2DKN6PY9s9TXEQTn6ZX3igCBQ/edit?usp=sharing'\n\ndados &lt;- gsheet2tbl(url)\nterr &lt;- unlist(dados[,2]) \nterr &lt;- ts( terr, start = 1900)\n\nts.plot(terr, ylab = 'No terremotos', xlab = 'Ano', lwd= 2)\n\n\n\n\nNote que o processo parece ser estacionário. Vamos explorar as funções de autocorrelação nas duas figuras abaixo. O correlograma mostra um comportamento tendendo para zero após 5 defasagens enquanto que o gráfico da função de autocorrelação parcial apresenta um único valor significativo. Temos evidências o modelo AR(1) pode ser adequado.\n\nacf(terr)\n\n\n\npacf(terr)\n\n\n\n\nA função arima(x, order = c(p,0,0)) estima os parâmetros do modelo \\(AR(p)\\) já considerando a deriva (caso contrário, utilize o argumento include.mean=FALSE). Abaixo apresentamos o modelo ajustado.\n\nmod &lt;- arima( terr, c(1,0,0))\nmod\n\n\nCall:\narima(x = terr, order = c(1, 0, 0))\n\nCoefficients:\n         ar1  intercept\n      0.5433    19.8907\ns.e.  0.0840     1.3180\n\nsigma^2 estimated as 36.7:  log likelihood = -318.98,  aic = 643.97\n\n\nPodemos verificar se o ajuste é adequado analisando os resíduos. Note que agora possuímos uma ferramente nova, o gráfico da função de autocorrelação parcial. Abaixo, os gráficos das funções de autocorrelação não apresentam valores significativos. O teste de Box-Pierce não rejeita a hipótese de um processo estacionário e o teste de Shapiro-Wilks não rejeita a hipótese de ruído branco gaussiano. Portano, temos evidências de que o modelo AR(1) com erros gaussianos é adequado para essa série.\n\nacf(mod$residuals)\n\n\n\npacf(mod$residuals)\n\n\n\nBox.test(mod$residuals)\n\n\n    Box-Pierce test\n\ndata:  mod$residuals\nX-squared = 0.92715, df = 1, p-value = 0.3356\n\nshapiro.test(mod$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  mod$residuals\nW = 0.97813, p-value = 0.09826\n\n\nAs estimativas encontradas foram \\(\\hat{\\phi}=0,5\\), \\(\\hat{\\nu}=36,7\\) e \\(\\hat{\\mu}=19\\). Podemos utilizar a função forecast do pacote de mesmo nome para fazer previsões. Note que a medida que o horizonte de previsão cresce, o modelo converge para a deriva.\n\nrequire(forecast)\n\nCarregando pacotes exigidos: forecast\n\n\nWarning: package 'forecast' was built under R version 4.3.1\n\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\nprev &lt;- forecast(mod, 10)\nplot(prev)"
  },
  {
    "objectID": "processo_linear_geral.html#médias-móveis",
    "href": "processo_linear_geral.html#médias-móveis",
    "title": "10  Modelos ARMA",
    "section": "10.3 Médias móveis",
    "text": "10.3 Médias móveis\nO processo \\[  x_t= \\varepsilon_t +\\sum_{j=1}^{q}\\theta_j\\varepsilon_{t-j},\\] onde \\(\\{\\varepsilon_t\\}\\) é um ruído branco, é denominado média móvel de ordem \\(q\\) (onde MA é a sigle de ). Note que esse modelo não está relacionado com o método de suavização de mesmo nome. Também podemos definir o modelo utilizando um polinômio de defasagens: \\[\\begin{align}x_t\n    =\\left(1+\\sum_{j=1}^{q}\\theta_jB^j\\right)\\varepsilon_t=\\theta(B)\\varepsilon_t\\end{align}\\]\nO processo MA\\((q)\\) é estacionário. De fato \\[E(x_t)=E\\left(\\varepsilon_t +\\sum_{j=1}^{q}\\theta_j\\varepsilon_{t-j}\\right)=0,\\] \\[Var(x_t)=\\nu\\left(1+\\sum_{j=1}^{q}\\theta_j^2\\right).\\] Agora, defina \\(\\psi_0=1\\), \\(\\psi_j=\\theta_j\\), para \\(j=1,\\ldots,q\\) e \\(\\psi_j=0\\) para \\(j&gt;q\\). Então, \\[\\begin{align}\nCov(x_t,x_{t-h})&=\\sum_{j=0}^q\\sum_{k=0}^q \\psi_j\\psi_k Cov\\left(\\varepsilon_{t-j},\\varepsilon_{t-k-h}\\right)\\\\&=\\nu\\sum_{k=0}^{q-h} \\psi_{k+h}\\psi_k\\\\\n&=\\nu\\left[\\theta_h+\\sum_{j=1}^{q-h}\\theta_j\\theta_{j+h}\\right]\\end{align}\\]\nA função de autocorrelação desse processo é dada por\n\\[\\rho(h)=\\left\\{\\begin{array}{ll}\\frac{\\theta_h+\\sum_{j=1}^{q-h}\\theta_j\\theta_{j+h}}{1+\\sum_{j=1}^q\\theta^2_j },&\\;\\;0&lt;h\\leq q\\\\0,&\\hbox{ caso contrário}\\end{array}\\right.\\]\nPortanto, o correlograma pode ser utilizado para determinar o valor que \\(q\\), uma vez que são esperadas \\(q\\) autocorrelações não nulas.\n\n10.3.1 Inversibilidade do processo de média móvel\nNem sempre o processo \\(MA(q)\\) é inversível. De fato, note que \\[x_t=\\varepsilon_t +\\theta\\varepsilon_{t-1}=(1+\\theta B)\\varepsilon_t\\Rightarrow x_t(1+\\theta B)^{-1}=\\varepsilon_t.\\] Sabemos que \\[\\theta^{-1}(B)=\\sum_{j=0}^{\\infty}\\theta^j B^j,\\] quando \\(|\\theta|&lt;1\\). Portanto, para este processo ser inversível é necessário que \\(|\\theta|&lt;1\\). Observe que\n\\[\\theta^{-1}(B)x_t=\\varepsilon_t\\Rightarrow x_t=\\sum_{j=1}^\\infty \\theta^j x_{t-j}+\\varepsilon_t,\\] logo, a inversa do processo MA\\((1)\\) é equivalente ao modelo AR(\\(\\infty\\)).\n\nProposition 10.2 O processo MA\\((q)\\) é inversível se e somente se o módulo das raízes do polinômio \\(\\theta(B)\\) forem maiores que um. \\(\\blacksquare\\)\n\n\n\n10.3.2 Autocorrelação parcial do MA(\\(q\\))\nConsidere que o processo MA\\((q)\\) inversível. Então, sabemos que \\[x_t=\\sum_{j=0}^\\infty\\psi_j x_{t-j}+\\varepsilon_t \\] existe e é estacionário. Defina \\[\\begin{align}\n\\hat{x}_{t+h}&=\\sum_{j=1}^p\\phi_j x_{t+h-j}\\\\\n\\hat{x}_{t}&=\\sum_{j=1}^p\\phi_j x_{t+j}\\\\\n\\end{align}\\]\nEntão, \\[\\begin{align}\\phi_{hh}&=Cov(x_{t+h}-\\hat{x}_{t+h},x_t-\\hat{x}_t)\\\\\n&=Cov\\left(x_{t+h}-\\sum_{j=1}^\\infty\\phi_j x_{t+h-j},x_t- \\sum_{j=1}^\\infty\\phi_j x_{t+j}\\right)\\\\&=Cov\\left(\\varepsilon_{t+h},x_t- \\sum_{j=1}^\\infty\\phi_j x_{t+j}\\right)\\\\&=-\\sum_{j=h}^\\infty\\psi_jCov(\\varepsilon_{t+h},x_{t+j})\\\\&=-\\sum_{j=h}^\\infty\\psi_jCov(\\varepsilon_{t+h},x_{t+j})\\end{align}\\] e, como o processo é inversível, \\(\\phi_{hh}\\) deve decair exponencialmente para zero.\n\nConsidere o processo MA(1) inversível. É possível mostrar que \\[Cov(\\varepsilon_t,x_{t+j})=\\nu\\theta^j\\] Sabemos que \\[x_t=\\sum_{j=1}^\\infty \\psi_jx_{t-j}+\\varepsilon_t,\\] onde \\(\\psi_j=\\theta^j\\). Então, \\[\\begin{align}\\phi_{hh}&=-\\sum_{j=h}^\\infty\\theta^j Cov(\\varepsilon_{t+h},x_{t+j})=-\\sum_{j=h}^\\infty\\theta^{2j} \\nu\\end{align}\\]\n\n\n\n10.3.3 Estimação dos parâmetros do MA(\\(q\\))\nSabemos que \\[f(x_1,\\ldots,x_n)=f(x_1)\\prod_{t=2}^{n}f(x_t|x_1,\\ldots,x_{t-1}).\\]\nConsidere inicialmente o modelo $MA(1)$ e considere que $\\varepsilon_0$ é conhecido. Então,\n$$x_1= \\varepsilon_1+\\theta\\varepsilon_0\\sim\\hbox{Normal}(\\theta \\varepsilon_0,\\nu ).$$\nAlém disso, após observar \\(x_1\\), podemos escrever \\[\\hat{\\varepsilon}_1=x_1-\\theta\\varepsilon_0.\\] Como \\[\\begin{align*}\nx_2|\\mathcal{D}_1\\sim\n    (\\varepsilon_2+\\theta\\varepsilon_1|\\varepsilon_0,x_1)\\sim\\varepsilon_1+\\theta\\hat{\\varepsilon}_1\\sim\\hbox{Normal}(\\theta\\hat{\\varepsilon}_1,\\nu).\n    \\end{align*}\\]\nDe um modo geral, fazendo \\[\\hat{\\varepsilon}_t = x_t-\\theta\\hat{\\varepsilon}_{t-1}\\] teremos \\[\\begin{align*}\n    (x_t|\\mathcal{D}_{t-1})&\\sim  \\hbox{Normal}(\\theta \\hat{\\varepsilon}_{t-1},\\nu)\n    \\end{align*}\\] e podemos escrever a verossimilhança,\n\\[L(\\theta,\\nu,\\varepsilon_0)=\\left(\\frac{1}{2\\pi\\nu}\\right)^{\\frac{n}{2}}\\exp\\left\\{-\\frac{1}{2\\nu}\\left[(x_1-\\theta\\varepsilon_0)^2+\\sum_{t=2}^n(x_t-\\theta\\hat{\\varepsilon}_{t-1})^2\\right]\\right\\}\\]\nPara um modelo \\(MA(q)\\), supomos que \\(\\varepsilon_{-q+1},\\ldots,\\varepsilon_{0}\\) são conhecidos. Fazendo \\[\\begin{equation}\n    \\hat{\\varepsilon}_{t}=x_t - \\sum_{j=1}^{q}\\theta_j\\hat{\\varepsilon}_{t-j},\n    \\end{equation}\\] e\n\\[\\begin{align}\n    \\lambda_t'=-(\\hat{\\varepsilon}_{t-1},\\ldots,\\hat{\\varepsilon}_{t-q})\n    \\end{align}\\] teremos que \\[x_{t}|\\mathcal{D}_{t-1}\\sim\\hbox{Normal}\\left(\\lambda_t'\\boldsymbol{\\theta},\\nu\\right)\\] onde \\(\\boldsymbol{\\theta}'=(\\theta_1,\\ldots,\\theta_q)\\). Logo, \\[\\begin{equation}\n    L(\\boldsymbol{\\theta},\\nu,\\varepsilon_{-q+1},\\ldots,\\varepsilon_0)\\propto \\left(\\frac{1}{\\nu}\\right)^{\\frac{n}{2}}\\exp\\left\\{-\\frac{1}{2\\nu}\\sum_{t=1}^n\\left(y_t -\\lambda_t'\\boldsymbol{\\theta}\\right) ^ 2\\right\\}\n    \\end{equation}\\]\nEm relação aos valores \\(\\varepsilon_{-q+1},\\ldots,\\varepsilon_0\\), temos duas estratégias de otimização: - Fazer \\(\\varepsilon_{-q+1}=\\cdots=\\varepsilon}_0=0\\): isto é equivalente a dizer que todos estes valores são iguais à sua média. - Estimar \\(\\varepsilon_{-q+1},\\ldots,\\varepsilon_0\\): isto altera o número de parâmetros para \\(2q+1\\). Neste caso, a estratégia anterior pode ser utilizada como valores iniciais para o otimizador."
  },
  {
    "objectID": "processo_linear_geral.html#modelos-não-estacionários",
    "href": "processo_linear_geral.html#modelos-não-estacionários",
    "title": "10  Modelos ARMA",
    "section": "10.4 Modelos não estacionários",
    "text": "10.4 Modelos não estacionários\nVimos no começo do curso que o passeio aleatório, dado por \\[x_t=x_{t-1}+\\varepsilon_t\\] onde \\(\\varepsilon_t\\) é um ruído branco, poderia gerar tendências aleatórias."
  },
  {
    "objectID": "tendencia.html#o-que-é-tendência",
    "href": "tendencia.html#o-que-é-tendência",
    "title": "6  Tendência",
    "section": "6.1 O que é tendência?",
    "text": "6.1 O que é tendência?\nDiz-se que uma série temporal observada possui tendência quando ela exibe um padrão de crescimento ou decrescimento em médio/longo prazo.\nA Figure 6.1 mostra a série do número mensal de acidentes com aeronaves, construída através dos dados diários mantidos pela Força Aérea Brasileira. Note uma tendência de decrescimento na série até meados de 2016, substituída então por uma tendência de crescimento.\n\nurl &lt;- 'https://www.dropbox.com/scl/fi/kq4jwbovu94u857238sus/N-mensal-de-acidentes-com-aeronaves-2013jan.csv?rlkey=n5pa45e7ht33houmiawdkjb09&dl=1'\n\nx &lt;- read.csv(url, h = T)\nacidentesFAB &lt;- ts( x, start = c(2013,1), frequency=12)\nts.plot(acidentesFAB, lwd = 2, xlab = 'Ano', ylab = 'No. acidentes')\n\n\n\n\n\n\nFigure 6.1: Número mensal de acidentes envolvendo aeronaves. Fonte: FAB"
  },
  {
    "objectID": "tendencia.html#tendência-aletória-e-tendência-determinística",
    "href": "tendencia.html#tendência-aletória-e-tendência-determinística",
    "title": "6  Tendência",
    "section": "6.2 Tendência aletória e tendência determinística",
    "text": "6.2 Tendência aletória e tendência determinística\nA tendência pode ser duas naturezas: determinística ou aleatória.\nA tendência aleatória é construída ao acaso. Considere, por exemplo, o passeio aleatório definido por \\(x_0=0\\) e \\(x_t = x_{t-1}+\\varepsilon_t\\), onde \\(\\varepsilon_t\\) é um ruído branco gaussiano com \\(\\nu=1\\). Já foi mostrado que \\(E(x_t)=0\\) e \\(Var(x_t)=t\\). A figura abaixo apresenta uma série simulada desse processo.\n\nset.seed(1)\nx = 0\nfor( t in 2:100) x[t] = x[t - 1] + rnorm(1,0,1)\nts.plot(x, lwd = 2)\n\n\n\n\nObserve que a série exibe um tendência, mas não há qualquer explicação para a sua exsitência, uma vez que este comportamento é fruto do acaso. Ainda, teremos que \\(E(x_t)=0\\), o que torna o padrão observado irrelevante.\nNa tendência determinística, há uma função T(.) que determina seu comportamento. Nesse caso, é assumido que\n\\[y_t = T(t) + \\varepsilon_t,\\] onde \\(\\varepsilon_t\\) é uma série estacionária com média \\(0\\) e variância \\(\\nu\\). Deste modo, \\(E(y_t)=T(t)\\), o que implica que \\(T(.)\\) representa o comportamento médio da série. O problema de estimar \\(T(.)\\) é denominado suavização.\nNa prática, é impossível determinar se uma tendência é aleatória ou determinística, cabendo ao estastístico procurar se há motivos para acreditar que está analisando o segundo tipo. A partir deste momento, toda tendência será considerada determinística."
  },
  {
    "objectID": "tendencia.html#o-modelo-de-tendência-polinomial",
    "href": "tendencia.html#o-modelo-de-tendência-polinomial",
    "title": "6  Tendência",
    "section": "6.3 O modelo de tendência polinomial",
    "text": "6.3 O modelo de tendência polinomial\nConsidere que a série temporal foi observada até o tempo \\(s\\). Então, a tendência é definida como uma função \\(T:(0,t]\\rightarrow \\mathbb{R}\\). O Teorema de Weierstrass afirma que, se \\(T\\) é contínua, então para qualquer \\(\\delta&gt;0\\), existe um polinômio \\(u(.)\\) tal que \\[|T(t)-u(t)|&lt;\\delta.\\] Isto quer dizer que \\(T(.)\\) sempre pode ser aproximada por um polinômio. Assim, para determinada ordem \\(p\\), é correto afirmar que\n\\[\\begin{equation}\n        y_t = \\beta_0 + \\sum_{j=1}^p \\beta_j t^j + \\varepsilon_t\n        \\end{equation}\\] onde \\(\\varepsilon_t\\) é uma série estacionária, é um modelo razoável para uma série temporal com tendência. Assumindo que \\(\\varepsilon_t\\) é um ruído branco gaussiano, tem-se o modelo de tendência polinomial de grau \\(p\\).\nFazendo \\(\\boldsymbol{f}_t'=(1,t,\\ldots,t^p)\\) , o modelo de tendência polinomial é reescrito como \\[\\boldsymbol{y}=\\boldsymbol{F}'\\boldsymbol{\\beta}+\\boldsymbol{\\varepsilon}\\] e inferências sobre \\(\\boldsymbol{\\beta}\\) e \\(\\nu\\) são feitas utilizando o modelo linear tradicional.\n\nExample 6.1 Considere o número anual de nascidos vivos no estado do Amazonas entre os anos 2000 e 20013:\n\nx &lt;- c( 67646 , 70252 , 70671 , 70751 , 71345 ,\n        73488 , 75584 , 73469 , 75030 , 75729 , \n        74188 , 76202 , 77434 , 79041)\n\nnascidos &lt;- ts(x, start =2000)\n\n\nts.plot(nascidos, lwd = 2, ylab = 'No. nascidos vivos')\nstats::acf(nascidos)\n\n\n\n\n\n\nFigure 6.2: Número de nascimentos anual no estado do Amazonas (Fonte: SINASC/SUS)\n\n\n\n\n\n\n\nFigure 6.3: Correlograma da série (Fonte: SINASC/SUS).\n\n\n\n\n\n\nVamos ajustar um modelo de tendência polinomial de ordem 1, ou seja\n\\[y_t=\\beta_0+\\beta_1 t + \\varepsilon_t\\] onde \\(t=1,\\ldots,14\\) representa os tempos \\(2000,\\ldots,2013\\).\n\ntempo &lt;- 1:14\nmod &lt;- lm( nascidos ~ poly(tempo, 1, raw = TRUE))\n\nAs estimativas de máxima verossimilhança para \\(\\beta_0\\) e \\(\\beta_1\\) são:\n\nmod$coefficients\n\n               (Intercept) poly(tempo, 1, raw = TRUE) \n                 68266.879                    715.178 \n\n\nou seja, \\[\\hat{T}(t)=\\hat{\\beta}+\\hat{\\beta}_1 t = 68.267+715 t\\] Os resíduos do modelo linear mod podem ser obtidos via função residuals. Abaixo, verificamos que a série dos resíduos oscila em torno de zero e que nenhuma autocorrelação parece ser relevante, o que dão indícios de que os erros são um ruído branco.\n\nres &lt;- residuals(mod)\nts.plot( res, main = '')\nacf(res, main = '')\n\n\n\n\n\n\nSérie dos resíduos\n\n\n\n\n\n\n\nCorrelograma dos resíduos\n\n\n\n\n\n\nAbaixo, o teste de Shapiro-Wilks não gera evidências contra a suposição de normalidade e o teste de Box-Pierce não gera evidências contra a hipótese de ruído branco.\n\nshapiro.test(res)\n\n\n    Shapiro-Wilk normality test\n\ndata:  res\nW = 0.96982, p-value = 0.8743\n\nBox.test(res)\n\n\n    Box-Pierce test\n\ndata:  res\nX-squared = 0.02156, df = 1, p-value = 0.8833\n\n\nÉ interessante notar que, para \\(t=1,\\ldots,14\\), \\[\\hat{T}(t)=\\hat{\\beta}_0+\\hat{\\beta}_1 t = \\hat{y}_t,\\] logo, os valores preditos do modelo são uma estimativa para a tendência nos pontos observados.\n\nts.plot( cbind( nascidos, fitted(mod)), col = 1:2, lwd = 2)\n\n\n\n\n\n\nLinha preta: série original. Linha vermelha: tendência estimada\n\n\n\n\n\n\n\n\n6.3.1 Previsão\nA previsão é realizada utilizando o modelo ajustado, estrapolando para um tempo não observado. Por exemplo a estimativa para 2014 (\\(t=15\\)) é\n\\[\\hat{T}(15)=\\hat{\\beta}+\\hat{\\beta}_1 15 = 78.992\\] (o valor real foi 81.145).\nÉ importante ressaltar que esse tipo de modelo é interessante para fazer inferências sobre a tendência, mas pode ser inadequado para previsões, uma vez que o polinômio é uma aproximação apenas para o intervalo observado.\n\n\n6.3.2 Seleção de modelos lineares\nO valor do Critério de Informação de Akaike (AIC) é dado por \\(-2L(\\hat{\\theta})+2k\\) onde \\(L\\) é a função de verossimilhança e \\(\\hat{\\theta}\\) e \\(k\\) são o estimador de máxima verossimilhança para \\(\\theta\\) e sua dimensão, respectivamente. O modelo com menor AIC é considerado mais adequado.\n\nConsidere o nível anual, em pés, do Lago Huron. Essa série já vem carregada no R sob o nome LakeHuron.\n\nts.plot(LakeHuron, lwd = 2, ylab = 'Nível (pés)')\nstats::acf(LakeHuron)\n\n\n\n\n\n\nFigure 6.4: Nível anual do Lago Huron, entre 1875 e 1972\n\n\n\n\n\n\n\nFigure 6.5: Correlograma da série\n\n\n\n\n\n\nVamos ajustar alguns modelos para tentar explica a tendêndia dessa série.\n\ntempo &lt;- 1 : length(LakeHuron)\nmod1 &lt;- lm( LakeHuron ~ poly(tempo, 1, raw = T))\nmod2 &lt;- lm( LakeHuron ~ poly(tempo, 2, raw = T))\nmod3 &lt;- lm( LakeHuron ~ poly(tempo, 3, raw = T))\nmod4 &lt;- lm( LakeHuron ~ poly(tempo, 4, raw = T))\nmod5 &lt;- lm( LakeHuron ~ poly(tempo, 5, raw = T))\nmod6 &lt;- lm( LakeHuron ~ poly(tempo, 6, raw = T))\n\nAIC(mod1)\n\n[1] 306.0957\n\nAIC(mod2)\n\n[1] 287.8407\n\nAIC(mod3)\n\n[1] 289.8391\n\nAIC(mod4)\n\n[1] 291.7127\n\nAIC(mod5)\n\n[1] 293.475\n\nAIC(mod6)\n\n[1] 291.7054\n\n\nEntre os modelos ajustados, o de ordem 2 foi aquele com o menor valor do AIC. Sua tendência estimada é\nO polinômio ajustado foi \\[\\hat{T}(t) = 581 -0,091 t + 0,001 t^2\\]\nAbaixo, apresentamos a análise de resíduos desse modelo.\n\nres &lt;- residuals(mod2)\nts.plot( res, main = '')\nabline(h = 0, lty = 2)\nacf(res, main = '')\n\n\n\n\n\n\nSérie dos resíduos\n\n\n\n\n\n\n\nCorrelograma dos resíduos\n\n\n\n\n\n\nOs resíduos parecem oscilar em torno de zero com um variância constante, mas o correlograma sugere que não temos um ruído branco. O teste de Box-Pierce, dado abaixo, confirma a nossa suspeita. Deste modo, este modelo não é adequado.\n\nBox.test(res)\n\n\n    Box-Pierce test\n\ndata:  res\nX-squared = 50.973, df = 1, p-value = 9.365e-13"
  },
  {
    "objectID": "tendencia.html#métodos-não-paramétricos-para-estimação-da-tendência",
    "href": "tendencia.html#métodos-não-paramétricos-para-estimação-da-tendência",
    "title": "6  Tendência",
    "section": "6.4 Métodos não paramétricos para estimação da tendência",
    "text": "6.4 Métodos não paramétricos para estimação da tendência\nO modelo de tendência polinomial é robusto quando relaxamos a necessidade do ruído branco ser gaussiano. Nesse sentido, as estimativas ainda são válidas, mas perdemos todos os testes de hipóteses.\nOs métodos não paramétricos independem da distribuição do ruído, sendo úteis para a análise exploratória.\n\n6.4.1 Médias Móveis\nO método das médias móveis consiste em obter \\(\\hat{T}(t)\\) através da média da série considerando os valores vizinhos à \\(y_t\\). Para o tempo \\(t\\) e \\(m=2h+1\\), com \\(h=1,2,\\ldots\\), considere o conjunto \\(\\mathcal{V}(m)_t=\\{t-h,\\ldots,t+h\\}\\). Defini-se a média móvel de ordem \\(m\\) (notamção \\(m\\)-MM) como \\[\\hat{T}_h(t)=\\frac{1}{m}\\sum_{ i = t-h}^{t+h}y_i,\\;h&lt;t&lt;n-h\\].\nPara compreender melhor esse estimador, considere que a relação entre pontos vizinhos é aproximadamente linear, ou seja, para qualquer \\(t\\in\\mathcal{V}(m)_t\\) existem \\(a_\\mathcal{V}\\) e \\(b_\\mathcal{V}\\) tais que \\[y_t\\approx a_\\mathcal{V}+b_\\mathcal{V}t+\\varepsilon_t,\\] onde \\(\\varepsilon_t\\) é considerado uma série temporal estacionária e ergódica. Então \\[\\begin{align}E(\\hat{T}(t))&=\\frac{a_\\mathcal{V}+b_\\mathcal{V}(t-h)+\\cdots+a_\\mathcal{V}+b_\\mathcal{V}t+\\cdots+a_\\mathcal{V}+b_\\mathcal{V}(t+h)}{m}\\\\&=a_\\mathcal{V}+b_\\mathcal{V}t\\end{align}\\] \\[Var(\\hat{T}(t))=\\frac{\\nu}{m}+\\frac{2}{m}\\sum_{j=1}^{2h}j\\gamma(j)\\] Observe que, como \\(T(.)\\) é determinística e os ruídos são estacionários e ergódicos, então \\(Var(\\hat{T}(t))\\) converge para zero quando \\(m\\rightarrow \\infty\\). Contudo, \\(T(.)\\) é localmente linear, logo \\(\\hat{T}\\) é um estimador razoável para valores baixos de \\(h\\). Este é um exemplo típico de trade off entre víes e variância, onde não é possível minimizar os dois simultaneamente.\nUtilizaremos a função ma(x,m), do pacote forecast para encontrar \\(m-\\)MM para a série `x\n\nExample 6.2 Abaixo, apresentamos a série anual histórica de nascidos vivos no Amazonas desde 1994 até 2021.\n\nx &lt;- c(47780, 47966, 49112, 56070, 57180, 62037,\n67646, 70252, 70671, 70751, 71345, 73488, 75584,\n73469, 75030, 75729, 74188, 76202, 77434, 79041,\n81145, 80097, 76703, 78066, 78087, 77622, 75635,\n78454)\n\nnascidos &lt;- ts(x, start =1994)\nts.plot(nascidos, lwd = 2, ylab = 'No. nascidos vivos')\n\n\n\n\n\nrequire(forecast)\n\nCarregando pacotes exigidos: forecast\n\n\nWarning: package 'forecast' was built under R version 4.3.1\n\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\noo &lt;- par( mfrow = c(2,2), mar = c(2,2,1,1))\nts.plot(nascidos, lwd = 2, ylab = 'No. nascidos vivos')\nlines( ma(nascidos,3) , col =2, lwd = 2)\nlegend('bottomright', legend = c('Série original','3-MM'),  fill = c(1,2,3), bty='n')\nts.plot(nascidos, lwd = 2, ylab = 'No. nascidos vivos')\nlines( ma(nascidos,5) , col =2, lwd = 2)\nlegend('bottomright', legend = c('Série original','5-MM'),  fill = c(1,2,3), bty='n')\nts.plot(nascidos, lwd = 2, ylab = 'No. nascidos vivos')\nlines( ma(nascidos,7) , col =2, lwd = 2)\nlegend('bottomright', legend = c('Série original','7-MM'),  fill = c(1,2,3), bty='n')\nts.plot(nascidos, lwd = 2, ylab = 'No. nascidos vivos')\nlines( ma(nascidos,9) , col =2, lwd = 2)\nlegend('bottomright', legend = c('Série original','9-MM'),  fill = c(1,2,3), bty='n')\n\n\n\npar(oo)\n\nConsidere a estimativa obtida pela média móvel de ordem 3.\n\ntendencia &lt;- ma(nascidos, 3)\ntendencia\n\nTime Series:\nStart = 1994 \nEnd = 2021 \nFrequency = 1 \n [1]       NA 48286.00 51049.33 54120.67 58429.00 62287.67 66645.00 69523.00\n [9] 70558.00 70922.33 71861.33 73472.33 74180.33 74694.33 74742.67 74982.33\n[17] 75373.00 75941.33 77559.00 79206.67 80094.33 79315.00 78288.67 77618.67\n[25] 77925.00 77114.67 77237.00       NA\n\n\nVamos estimar o ruído da série (e eliminar as coordenadas vazias)\n\nruido &lt;- nascidos - tendencia\nruido &lt;- ruido[is.na(ruido) == F]\n\nAbaixo, apresentamos as principais estatísticas sobre os resíduos. A série histórica dos resíduos oscila em torno de zero e não há motivos para suspeitar de que sua variância é constante. O correlograma apresenta autocorrelações baixas, como o esperado em um ruído branco. O teste de Shapiro-Wilks não dá evidências contra normalidade, o que suporta a hipótese de ruído branco gaussiano. O teste de Box-Pierce apresenta um p-valor de 0,04 e, em conjunto com as demais evidências, vamos considerá-lo significativo ao nível de 4%.\n\nts.plot(ruido)\nabline( h = 0, lty = 2)\nabline( h = 2*sd(ruido), lty=2)\nabline( h = -2*sd(ruido), lty=2)\n\n\n\nacf(ruido)\n\n\n\nshapiro.test(ruido)\n\n\n    Shapiro-Wilk normality test\n\ndata:  ruido\nW = 0.9711, p-value = 0.6521\n\nBox.test(ruido)\n\n\n    Box-Pierce test\n\ndata:  ruido\nX-squared = 4.1804, df = 1, p-value = 0.04089\n\n\n\\(\\blacksquare\\)\n\nAté o momento, foi considerado que a ordem da média móvel é escrita como \\(m=2h+1\\), ou seja, a ordem é sempre ímpar. Sem Para definir a média móvel para uma ordem par, perda de generalidade, assuma que \\(m=4\\). Como não é possível escolher um número igual de vizinhos à \\(y_t\\), tem-se duas possibilidades para \\(\\mathcal{V}(4)_t\\): \\[\\mathcal{V}'(4)_t=\\{y_{t-1},y_t,y_{t+1},y_{t+2}\\}\\] e \\[\\mathcal{V}''(4)_t=\\{y_{t-2},y_{t-1},y_t,y_{t+1}\\}.\\] Para cada possibilidade, tem-se \\[\\hat{T}'(t)=\\frac{1}{4}\\sum_{i=t-1}^{t+2}y_{i}\\] e \\[\\hat{T}''(t)=\\frac{1}{4}\\sum_{i=t-2}^{t+1}y_{i}.\\] A média móvel 2-MM será definida por \\[\\hat{T}(t)=\\frac{T'(t)+T''(t)}{2}\\] Observe que a média agora é ponderada, uma vez que\n\\[\\hat{T}(t)=\\frac{y_{t-2}}{8}+\\frac{y_{t-1}}{4}+\\frac{y_{t}}{4}+\\frac{y_{t+1}}{4}+\\frac{y_{t+2}}{8}.\\]\nPara o caso de \\(m=2h\\) com \\(h=1,2,\\ldots\\), defini-se \\(m\\)-MM por \\[\\begin{align}\\hat{T}(t)&=\\frac{1}{2}\\left[\\frac{1}{m}\\sum_{i=t-h}^{t+h-1}y_i+\\frac{1}{m}\\sum_{i=t-h+1}^{t+h}y_i\\right]\\\\&=\\frac{y_{t-h}}{2m}+\\frac{1}{m}\\sum_{i=t-h+1}^{t+h-1}y_i+\\frac{y_{t+h}}{2m}.\\end{align}\\] Note que o argumento de \\(\\hat{T}(t)\\) é aproximadamente não viciado para \\(m\\) pequeno não se altera, uma vez que os pesos para os tempos \\(t-j\\) e \\(t+j\\) são simétricos\nO estimador para tendência conhecido como média móvel ponderada de ordem \\(m\\) é dado por\n\\[\\hat{T}(t)=\\sum_{i=t-h}^{t+h} w_i y_{i},\\] com \\(w_i&gt;0\\), \\(w_{t-j}=w_{t+j}\\) e \\(\\sum_{i=t-h}^{t+h}w_i=1\\). O método tradicional é obtido fazendo \\(w_i=1/m\\).\nPor último, como as primeiras e últimas observações são removidas, esses métodos não são os mais recomendados.\n\nOutras médias móveis\nÉ importante ressaltar que os métodos estatísticos são ferramentas universais e que geralmente sofrem modificações ao serem aplicados em outras áreas. Deste modo, existem outras definições de médias móveis que podem causar confusão.\nEm epidemiologia por exemplo, a média móvel de ordem \\(m\\) é definida por \\[\\hat{T}(t)=\\frac{1}{m}\\sum_{j=1}^m y_{t-j+1}\\] ou seja, a soma dos valores mais recentes em relação à \\(t\\). Observe que o contexto é diferente: em uma epidemia por exemplo, deseja-se estimar \\(T(t)\\) onde \\(t\\) é o tempo mais recente e, em geral, se utiliza o 7-MM tirando a média dos últimos 7 dias. A mesma lógica vale para o mercado financeiro, que tira a média dos últimos 5 dias de preço de fechamento.\nAinda no mercado financeiro, o importante é captar a mudança da tendência o mais rápido possível. Deste modo, utiliza-se uma média móvel ponderada definida por \\[\\hat{T}(t)=\\frac{2}{m(m+1)}\\sum_{j=1}^m(m-j+1)y_{t-j+1}.\\] Na definição acima, o último preço da ação, dado por \\(y_t\\), é o valor mais imporante e por isso recebe o maior peso. Note que os pesos não são simétricos."
  },
  {
    "objectID": "tendencia.html#suavização-do-gráfico-de-dispersão-estimada-localmente-loess",
    "href": "tendencia.html#suavização-do-gráfico-de-dispersão-estimada-localmente-loess",
    "title": "6  Tendência",
    "section": "6.5 Suavização do gráfico de dispersão estimada localmente (loess)",
    "text": "6.5 Suavização do gráfico de dispersão estimada localmente (loess)\nNo método de suavisação do gráfico de dispersão, deseja-se estimar \\(f(x)=E(y|x)\\), através da coleção \\((y_1,x_1),\\ldots,(y_n,x_n)\\), para um valor qualquer de \\(x\\).\nA estimativa \\(\\hat{f}\\) para o ponto \\(x'\\) é calculada considerando os seguintes passos:\n\nFixe um valor inteiro positivo \\(q\\leq n\\).\nDentro do conjunto \\(x_1,\\ldots,x_n,\\) encontre os \\(q\\) valores mais próximos de \\(x'\\) (via distância euclidiana). Denote este conjunto por \\(\\mathcal{V}\\) e denote por \\(d\\) a maior distância encontrada.\nPara cada \\(x_1,\\ldots,x_n\\) seja \\[v_j(x')=\\left\\{\\begin{array}{ll}\\left(1-\\left| \\frac{x_j-x'}{d}\\right|^3\\right)^3&,\\;\\;\\hbox{se }|x_j-x|\\leq d\\\\ 0,&\\hbox{caso contrário}\\end{array}\\right.\\] o peso associado à \\(x_j\\) (valores próximos de \\(x'\\) receberão o peso máximo e valores afastados recebem menor peso)\nAjuste o modelo de regressão ponderado, minimizando \\[\\sum_{i=1}^n v_i(x')\\left(y_i - \\sum_{j=0}^p \\beta_jx^j\\right)^2\\]\nEstime \\(f(x')\\) por \\[\\hat{f}(x')=\\sum_{j=0}^p \\hat{\\beta}_jx^j \\]\n\nOberve que este método pode ser utilizado para estimar a tendência da série. Abaixo, vamos analisar a série de taxa de desemprego mensal, entre março de 2002 e dezembro de 2015.\n\nurl &lt;- 'https://www.dropbox.com/s/rmgymzsic99qawd/desemprego.csv?dl=1'\n\nbanco &lt;- read.csv(url, sep = ';', h = F)\n\ndesemprego&lt;- ts( banco$V2, start = c(2002,3), frequency=12)\n\nts.plot(desemprego, ylab = 'Taxa de desemprego')\n\n\n\nacf(desemprego, lag = 30)\n\n\n\n\nVamos estimar a tendência\n\n# criando a variável regressora\ntempo &lt;- 1 : length(desemprego)\n\n# aplicando o loess\nlw &lt;- loess( desemprego ~ tempo)\n\n# transformando o valor predito em uma série temporal\n\nfit &lt;- ts(lw$fitted, start = start(desemprego), frequency = frequency(desemprego) )\n\n# gráfico da tendência estimada\n\nts.plot( desemprego, ylab = 'Taxa de desemprego' , lwd = 2)\nlines(fit, lwd = 2, col = 'tomato')\nlegend('topright', c('Observado','Ajustado'),fill=c(1,'tomato'), bty='n')\n\n\n\n\nVamos eliminar a tendência estimada e avaliar o restante.\n\nyt &lt;- desemprego - fit\n\nts.plot(yt)\n\n\n\nacf(yt)\n\n\n\n\nFica claro o comportamento sazonal, o que implica que o restante não é uma série estacionária."
  },
  {
    "objectID": "processo_linear_geral.html#modelo-autorregressivo-com-médias-móveis",
    "href": "processo_linear_geral.html#modelo-autorregressivo-com-médias-móveis",
    "title": "10  Modelos ARMA",
    "section": "10.4 Modelo autorregressivo com médias móveis",
    "text": "10.4 Modelo autorregressivo com médias móveis\nO modelo autorregressivo com médias móveis (ARMA) é dado por \\[\\begin{equation}\n    x_t= \\sum_{j=1}^p \\phi_j x_{t-1} + \\varepsilon_t + \\sum_{k=1}^{q}\\theta_k\\varepsilon_{t-k},\n    \\end{equation}\\] onde \\((p,q)\\) são as ordens da parte autorregressiva e das médias móveis, respectivamente. Definimos \\(ARMA(0,q)=MA(q)\\) e, de modo análogo, definimos \\(ARMA(p,0)=AR(p).\\)\nUtilizando o operador defasagem teremos \\[\\begin{align}\n        x_t&= \\sum_{j=1}^p \\phi_j x_{t-1} + \\varepsilon_t + \\sum_{k=1}^{q}\\theta_k\\varepsilon_{t-k} \\\\\n        &=\n        \\sum_{j=1}^p \\phi_j B^jx_{t} + \\varepsilon_t + \\sum_{k=1}^{q}\\theta_kB^k\\varepsilon_{t}\n        \\end{align}\\] o que implica em \\[\\begin{align*}\n        \\underbrace{\\left(1-\\sum_{j=1}^p \\phi_j B^j\\right)}_{\\phi(B)}x_t = \\underbrace{\\left(1-\\sum_{j=1}^p \\theta_j B^j\\right)}_{\\theta(B)}\\varepsilon_{t}\n        \\end{align*}\\]\nO modelo ARMA será estacionário se o módulo das raízes de \\(\\phi(B)\\) forem maiores que um e será inversível se o mesmo ocorrer com o módulo das raízes de \\(\\theta(B)\\).\n\n10.4.1 Função de autocorrelação\nA função de auto covariância cruzada é dada por \\[\\begin{equation}\n      \\gamma_{x\\varepsilon}(h)=E(\\varepsilon_t x_{t-h}).\n    \\end{equation}\\] Note que \\(x_{t-h}\\) depende apenas dos ruídos que ocorreram até o tempo \\(t-h\\). Portanto \\[\\begin{align}\n     \\gamma_{x\\varepsilon}(h)=0,&\\;\\; h&gt;0 \\\\\n     \\gamma_{x\\varepsilon}(h)\\neq 0,&\\;\\; h\\leq 0 \\\\\n    \\end{align}\\]\nConsidere o processo ARMA(1,1), dado por \\[x_t = \\varepsilon_{t} + \\phi x_{t-1} + \\theta\\varepsilon_{t-1}.\\]\nTeremos que \\[\\begin{align*}\n\\gamma_{x\\varepsilon}(0)&=E(x_t\\varepsilon_t) = E( \\left[\\varepsilon_t + \\phi x_{t-1} + \\theta\\varepsilon_{t-1} \\right]\\varepsilon_t)\\\\\n&=\\underbrace{E(\\varepsilon_t^2)}_{\\nu}+ \\phi\\underbrace{ E(x_{t-1}\\varepsilon_t)}_{\\gamma_{x\\varepsilon}(1)=0} +\\theta \\underbrace{E(\\varepsilon_{t}\\varepsilon_{t-1})}_{=0} \\\\\n&=\\nu\n\\end{align*}\\]\nAgora, multiplicando a equação do modelo ARMA(1,1) em ambos os lados por \\(x_{t-h}\\), e aplicando a esperança teremos\n\\[\\begin{equation}\n    \\underbrace{E(x_{t-h}x_t)}_{\\gamma(h)} = \\underbrace{E(x_{t-h}\\varepsilon_{t})}_{\\gamma_{x\\varepsilon}(h)} + \\phi \\underbrace{E(x_{t-1}x_{t-h})}_{\\gamma(h-1)} + \\theta \\underbrace{E(x_{t-h}\\varepsilon_{t-1})}_{\\gamma_{x\\varepsilon}(h-1)}.\n    \\end{equation}\\]\nFazendo \\(h=0\\) na equação acima, teremos \\[\\gamma(0)=\\phi\\gamma(1) + \\theta\\gamma_{x\\varepsilon}(-1)\\] e, como \\[\\gamma_{x\\varepsilon}(-1)=E(x_{t+1}\\varepsilon_t)=\\]\nFazendo \\(h=1\\) teremos \\[\\gamma(1)=\\phi\\gamma(0)+\\theta\\nu\\]\nPara qualquer \\(h\\geq 2\\),\n\\[\\gamma(h)=\\phi\\gamma(h-1),\\] ou ainda \\[\\gamma(h)=\\phi^{h-1}\\gamma(1)=\\phi^h\\gamma(0)+\\theta\\nu.\\] Assim, \\[\\rho(h)=\\left\\{\\begin{array}{ll}\\end{array}\\right.\\]\nNote que a primeira auto covariância depende do parâmetro de média móvel.\n\nNote  ainda que após a primeira defasagem, a função de auto covariância se comporta como um modelo AR(1) tradicional: com um decaimento exponencial (podendo ser alternado ou não).\n\\end{frame}"
  },
  {
    "objectID": "processo_linear_geral.html#modelos-integrados",
    "href": "processo_linear_geral.html#modelos-integrados",
    "title": "10  Modelos ARMA",
    "section": "10.5 Modelos integrados",
    "text": "10.5 Modelos integrados\n\n10.5.1 O operador difereNça\nVimos no começo do curso que o passeio aleatório, dado por \\[x_t=x_{t-1}+\\varepsilon_t\\] onde \\(\\varepsilon_t\\) é um ruído branco, poderia gerar tendências aleatórias. Note que o passeio aleatório nada mais é que o processo AR(1) não estacionário. Podemos escrever esse modelo como \\[x_t-x_{t-1}=\\varepsilon_t\\Rightarrow(1-B)x_t=\\varepsilon_t\\Rightarrow\\Delta x_t=\\varepsilon_t\\] Acima,\\(\\Delta=1-B\\) é denominado operador diferença. Observe que a série \\[y_t=\\Delta x_t=\\varepsilon_t\\] é um ruído branco. Desse modo, o operador elimina a tendência aleatória gerada pelo passeio.\nAgora, considere que \\[x_t=T(t)+\\varepsilon_t,\\] onde \\(T(t)\\) é uma tendência localmente linear com inclinção constante, ou seja, para qualquer \\(s\\) na vizinhança de \\(t\\), \\[T(s)=\\mu_t+b s.\\] Então \\[\\Delta x_t= \\mu_t+bt-(\\mu_t+b(t-1))+\\varepsilon_t=b+\\varepsilon_t\\] e \\(y_t=\\Delta x_t=b+\\varepsilon_t\\) será um ruído branco com deriva.\nPor último, para qualquer \\(s\\) na vizinhança de \\(t\\), suponha que \\[T(s)=\\mu_t+b_t s.\\] Nesse caso, \\[\\Delta x_t= b_t+\\varepsilon_t\\] logo, o processo \\(y_t=\\Detal x_t\\) não é um ruído branco. Contudo, como é usual que \\(b_t\\approx b_{t+1}\\), teremos\n\\[\\Delta y_t= \\Delta x_t - \\Delta x_{t-1}=b_t-b_{t-1}+\\varepsilon_t-\\varepsilon_{t-1}\\approx \\varepsilon_t-\\varepsilon_{t-1}\\]\nlogo, aplicar o operador diferença pela segunda vez gera um ruído branco. Note que\n\\[\\Delta y_t=\\Delta (x_t-x_{t-1})=\\Delta(1-B)x_t=\\Delta^2 x_t.\\] Então, aplicar o operador diferença algumas vezes pode eliminar a tendência da série, gerando um ruído branco.\n\nConsidere o modelo \\[y_t=\\Delta x_t=\\varepsilon_t,\\] onde \\(\\varepsilon_t\\) é um ruído branco. Então, para \\(\\mathcal{D}_t=\\{x_1,\\ldots,x_t\\}\\), \\[E(y_{t+h}|\\mathcal{D}_t)=0\\Rightarrow E(x_{t+h}|\\mathcal{D}_t)=E(x_{t+h-1}|\\mathcal{D}_t)\\] e, como \\(E(x_t|\\mathcal{D}_t)=x_t\\), teremos \\[E(x_{t+h}|\\mathcal{D}_t)=x_t.\\] Portanto, esse modelo tem como previsão para qualquer horizonte o último valor observado, o que é equivalente ao modelo de suavização exponencial com \\(\\alpha=1\\).\nConsidere agora o modelo \\[y_t=\\Delta^2 x_t=\\varepsilon_t,\\] onde \\(\\varepsilon_t\\) é um ruído branco. Observe que \\[E(y_{t+1}|\\mathcal{D}_t)=0\\Rightarrow E(x_{t+1}|\\mathcal{D}_t)=x_t + (x_t-x_{t-1}).\\] É simples mostrar por indução que \\[E(x_{t+h}|\\mathcal{D}_t)=x_t+ h(x_t-x_{t-1}),\\] que é equivalente ao método de Holt com nível \\(m_t=x_t\\) e inclinação \\(b_t=(x_t-x_{t-1})\\).\n\\(\\blacksquare\\).\n\n\n\n10.5.2 Modelo autorregressivo integrado\nConsidere agora uma série temporal \\(x_t\\) na qual foram necessárias \\(d\\) diferenças para eliminar a tendência. Contudo, a série \\(y_t=\\Delta^d x_t\\) resultante é um ruído branco, mas sim um processo AR(\\(p\\)). Teremos\n\\[y_t=\\sum_{j=1}^p \\phi_j y_{t-j}+\\varepsilon_t \\Rightarrow \\left(1-\\sum_{j=1}^p \\phi_jB^j\\right)y_t=\\varepsilon_t\\] ou, de modo equivalente, \\[\\left(1-\\sum_{j=1}^p \\phi_jB^j\\right)\\Delta^d x_t=\\varepsilon_t.\\] O modelo acima é denominado autorregressivo integrado, e sua notação é ARI\\((p,d)\\), onde \\(p\\) é a ordem do modelo autorregressivo resultante das \\(d\\) diferenças da série original.\nAs estimativas são obtidas criando a série \\(y_t=\\Delta^d x_t\\) e estimando os parâmetros do modelo AR(\\(p\\)) resultante.\nA previsão do processo ARI(\\(p,d\\)) pode facilmente ser realizada a partir do modelo AR(\\(p\\)). Por exemplo, para \\(y_t=\\Delta x_t\\),\n\\[\\begin{align}E(y_{t+1}|\\mathcal{D}_t)&=\\sum_{j=1}^p \\phi_j y_{t-j}\\\\\\end{align}\\] o que implica em \\[\\begin{align} E(x_{t+1}|\\mathcal{D}_t)=x_t+\\sum_{j=1}^p\\phi_j(x_{t-j}-x_{t-j-1})\\end{align}\\]\n\nExample 10.10 Previsão para o modelo ARI(1,1)\nSeja \\(x_t\\) uma série temporal segundo o processo ARI(1,1). Então, \\(\\Delta y_t\\) é um processo AR(1) e \\(E(y_{t+h}|\\mathcal{D}_t)=\\phi^h y_t\\). Disto, notemos que\n\\[E(x_{t+1}-x_t|\\mathcal{D}_t)=\\phi (x_{t}-x_{t-1})\\Rightarrow E(x_{t+1}|\\mathcal{D}_t)=x_t+\\phi(x_t-x_{t-1}),\\] e \\[\\begin{align}&E(x_{t+2}-x_{t+1}|\\mathcal{D}_t)=\\phi^2 (x_t-x_{t-1})\\\\&\\Rightarrow E(x_{t+2}|\\mathcal{D}_t)=\\phi^2 (x_{t}-x_{t-1})+E(x_{t+1}|\\mathcal{D}_t)\\\\ &\\Rightarrow E(x_{t+2}|\\mathcal{D}_t)=\\phi^2 (x_{t}-x_{t-1})+\\phi (x_{t}-x_{t-1})+x_t.\\end{align}\\] Por indução, é imediato que \\[E(x_{t+h}|\\mathcal{D}_t)=x_t+\\sum_{j=1}^h \\phi^{j}(x_t-x_{t-1}).\\] Identificando \\(x_t\\) como a estimativa para nível da série no tempo \\(t\\) e \\((x_t-x_{t-1})\\) como a inclinação da tendência, temos que a função de previsão é um modleo com tendência amortecida.\n\\(\\blacksquare\\)\n\n\n\n10.5.3 Modelo autorregressivo sazonal\nSeja \\[x_t = g(t)+\\varepsilon_t,\\] onde \\(g(.)\\) é uma função períodica com período \\(p\\) e \\(\\varepsilon_t\\) um rúido branco. Note que \\[x_t-x_{t-p}=g(t)-g(t-p)+\\varepsilon_t-\\varepsilon_{t-p}=\\varepsilon_t-\\varepsilon_{t-p},\\] logo, a diferença acima gerou uma série estacionária. Observe que \\[x_t-x_{t-p}=(1-B^p)x_t.\\] O operador \\(1-B^p\\) é conhecido como diferença sazonal.\nSem perda de generalidade, assuma que \\(D=3\\). Então, a relação entre \\(x_t\\) e \\(x_{t-3}\\) pode ser obtida usando a seguinte transformação:\n\\[\\left(\\begin{array}{c}x_t\\\\ x_{t-1}\\\\ x_{t-2}\\end{array}\\right)=\\left(\\begin{array}{cc}0 & 0 & 1\\\\ 1 & 0 & 0\\\\ 0& 1 & 0\\end{array}\\right)\\left(\\begin{array}{c}x_{t-1}\\\\ x_{t-2}\\\\ x_{t-3}\\end{array}\\right)=\\left(\\begin{array}{cc}0 & 0 & 1\\\\ 1 & 0 & 0\\\\ 0& 1 & 0\\end{array}\\right)^2\\left(\\begin{array}{c}x_{t-4}\\\\ x_{t-5}\\\\ x_{t-6}\\end{array}\\right)\\] A matriz acima é denominada permutação, e já foi utilizada anteriormente. Para o caso geral,\n\\[\\left(\\begin{array}{c}x_t\\\\ x_{t-1}\\\\ x_{t-2}\\end{array}\\right)=\\left(\\begin{array}{cc}0 & 0 & 1\\\\ 1 & 0 & 0\\\\ 0& 1 & 0\\end{array}\\right)^h\\left(\\begin{array}{c}x_{t-(3h-2)}\\\\ x_{t-(3h-1)}\\\\ x_{t-3h}\\end{array}\\right)\\] ou ainda, \\[x_t=\\sum_{j=1}^3 c_j\\lambda_j^h x_{t-(3h-j+1)},\\] onde \\(\\lambda_j\\) são os autovalores da matriz acima. Acontece que \\[\\left|\\left(\\begin{array}{cc}0 & 0 & 1\\\\ 1 & 0 & 0\\\\ 0& 1 & 0\\end{array}\\right)-\\lambda \\textbf{I}_3\\right|=0\\Rightarrow \\lambda^3=1\\] Teremos que \\(\\lambda=1\\) é a solução real, enquanto que \\[\\lambda= \\exp\\left\\{\\frac{\\pi }{3}i\\right\\}=\\cos\\left(\\frac{2\\pi }{3}\\right)+i\\sin\\left(\\frac{2\\pi}{3}\\right).\\] e \\[\\lambda= \\exp\\left\\{\\frac{2\\pi }{3}i\\right\\}=\\cos\\left(\\frac{4\\pi }{3}\\right)+i\\sin\\left(\\frac{4\\pi k}{3}\\right).\\] são as soluções complexas.\nNa prática, para qualquer período \\(p\\), o autovalores existirão \\(p-1\\) autovalores da matriz de permutação satisfazendo \\[\\lambda= \\cos(2\\pi k/p)+i\\sin(2\\pi k/p),\\] com \\(k=1,\\ldots,D-1\\).\nDizemos que \\(x_t\\) é um processo autorregressivo sazonal se \\(y_t=(1-B^p)^Dx_t\\) é um processo autorregressivo.\nAssim como no caso do modelo integrado, os parâmetros são estimados a partir do modelo AR(\\(P\\)) considerando a amostra \\(y_t=(1-B^{p})^Dx_t\\).\nA previsão é\n\nExample 10.11 Seja \\(x_t\\) um processo autorregressivo sazonal, de período 12. Então,\n\\[(1-B^{12})x_t=\\Phi(1-B^{12})x_{t-1}+\\varepsilon_t,\\] ou ainda, \\[x_t=x_{t-12}+\\Phi x_{t-1}-\\Phi x_{t-13}+\\varepsilon_t\\]"
  },
  {
    "objectID": "processo_linear_geral.html#o-modelo-arma",
    "href": "processo_linear_geral.html#o-modelo-arma",
    "title": "10  Modelos ARMA",
    "section": "10.6 O modelo ARMA",
    "text": "10.6 O modelo ARMA\nO modelo auto regressivo de média móvel é dado por \\[  x_t= \\sum_{j=1}^p \\phi_j x_{t-1}  + \\sum_{k=1}^{q}\\theta_k\\varepsilon_{t-k}+ \\varepsilon_t,\n\\] onde \\((p,q)\\) são as ordens da parte autorregressiva e da média móvel, respectivamente. Definimos o modelo de média móveis como \\(ARMA(0,q)\\) e o autorregressivo como \\(AR(p).\\)\nUtilizando o operador defasagem teremos \\[\\begin{align}\n        x_t-\\sum_{j=1}^p \\phi_j x_{t-1}&= \\varepsilon_t + \\sum_{k=1}^{q}\\theta_k\\varepsilon_{t-k} \\\\\n        &\\Rightarrow\n        \\underbrace{\\left(1-\\sum_{j=1}^p \\phi_j B^j\\right)}_{\\phi(B)}x_{t} =  \\underbrace{\\left(1+\\sum_{k=1}^{q}\\theta_kB^k\\right)}_{\\theta(B)}\\varepsilon_{t}.\n        \\end{align}\\]\nO modelo ARMA será estacionário se o módulo das raízes de \\(\\phi(B)\\) forem maiores que um e será inversível se as raízes de \\(\\theta(B)\\) também o forem.\n\n10.6.1 A função de autocorrelação\nA função de autocovariância cruzada é dada por \\[\\begin{equation}\n      \\gamma_{x\\varepsilon}(h)=E(\\varepsilon_t x_{t-h}).\n    \\end{equation}\\] Note que \\(x_{t-h}\\) depende apenas dos ruídos que ocorreram até o tempo \\(t-h\\). Portanto \\[\\begin{align}\n     \\gamma_{x\\varepsilon}(h)=0,&\\;\\; h&gt;0 \\\\\n     \\gamma_{x\\varepsilon}(h)\\neq 0,&\\;\\; h\\leq 0 \\\\\n    \\end{align}\\]\n\nExample 10.12 Considere o processo ARMA(1,1), dado por \\[x_t = \\varepsilon_{t} + \\phi x_{t-1} + \\theta\\varepsilon_{t-1}.\\] Teremos que \\[\\begin{align*}\n\\gamma_{x\\varepsilon}(0)&=E(x_t\\varepsilon_t) = E( \\left[\\varepsilon_t + \\phi x_{t-1} - \\theta\\varepsilon_{t-1} \\right]\\varepsilon_t)\\\\\n&=\\underbrace{E(\\varepsilon_t^2)}_{\\nu}+ \\phi\\underbrace{ E(x_{t-1}\\varepsilon_t)}_{\\gamma_{x\\varepsilon}(1)=0} -\\theta \\underbrace{E(\\varepsilon_{t}\\varepsilon_{t-1})}_{=0} \\\\\n&=\\nu\n\\end{align*}\\]\n\\(\\blacksquare\\)\n\nConsidere o processo ARMA\\((1,1)\\), dado por \\[x_t = \\varepsilon_{t} + \\phi x_{t-1} + \\theta\\varepsilon_{t-1}.\\] Multiplicando ambos os lados por \\(x_{t-h}\\), e aplicando a esperança teremos \\[\n    \\underbrace{E(x_{t-h}x_t)}_{\\gamma(h)} = \\underbrace{E(x_{t-h}\\varepsilon_{t})}_{\\gamma_{x\\varepsilon}(h)} + \\phi \\underbrace{E(x_{t-1}x_{t-h})}_{\\gamma(h-1)} + \\theta \\underbrace{E(x_{t-h}\\varepsilon_{t-1})}_{\\gamma_{x\\varepsilon}(h-1)}\n    \\]\nFazendo \\(h=0\\) na equação acima teremos \\[\\gamma(0)=\\phi\\gamma(1) + \\theta\\gamma_{x\\varepsilon}(-1).\\]\nFazendo \\(h=1\\) teremos \\[\\gamma(1)=\\phi\\gamma(0)+\\theta\\nu.\\]\nPara qualquer \\(h\\geq 2\\),\n\\[\\gamma(h)=\\phi\\gamma(h-1),\\] ou ainda \\[\\gamma(h)=\\phi^{h-1}\\gamma(1),\\] e \\[\\rho(h)=\\phi^h+\\nu\\theta\\phi^{h-1}\\frac{\\nu}{\\gamma(0)}.\\]\nNote que a função de autocovariância se comporta como um modelo AR(1) tradicional, com um decaimento exponencial (podendo ser alternado ou não).\nPara o caso geral: - As \\(q\\) primeiras autocorrelações dependerão dos parâmetros \\(\\phi_1,\\ldots,\\phi_p\\) e \\(\\theta_1,\\ldots,\\theta_q\\). - Se \\(q&lt;p\\), toda a função de autocorrelação consistirá de uma mistura de exponenciais ou cossenos amortecidos, dependendo das raízes do polinômio \\(\\phi(B)\\). - Se \\(q\\geq p\\), então as \\(q-p+1\\) primeiras autocorrelações não seguirão o padrão descrito acima.\nPara construir a função de verossimilhança, observe que \\[y_t=\\phi(B)x_t=\\theta(B)\\varepsilon_t,\\] ou seja, \\(y_1,\\ldots,y_n\\) é um modelo de média móvel e sua função de verossimilhança pode ser construída desse a partir desse modelo.\n\n\n10.6.2 Modelos ARMA integrados e sazonais\nDizemos que \\(x_t\\) é um processo autorregressivo integrado de médias móveis se\n\\[\\phi(B)\\Delta^d x_t =\\theta(B)\\varepsilon_t,\\] e denotamos esse modelo por ARIMA\\((p,d,q)\\). A previsão desse modelo é dada por \\[\\hat{y}_t(h)=E(y_{t+h}|y_t,\\ldots y_{t-(p+d)}).\\]\nSeja \\(x_t\\) uma série temporal sazonal com período \\(s\\). Considere que o processo \\({x_t,x_{t-s},x_{t-2s}}\\) é um ARIMA\\((P,D,Q)\\): \\[\\Phi(B)(1-B^P)^Dx_t = \\Theta(B)\\eta_t,\\] e que o processo \\(\\eta_t\\sim \\hbox{ARIMA}(p,d,q)\\). Então, \\[\\theta(B)(1-B)^d\\eta_t = \\theta(B)\\varepsilon_t\\Rightarrow \\eta_t = \\left[\\theta(B)(1-B)^d\\right]^{-1}\\theta(b)\\varepsilon_t.\\] Portanto, teremos o modelo \\[\\theta(B)\\Phi(B)(1-B^P)^D(1-B)^dx_t = \\Theta(B)\\theta(B)\\eta_t,\\]\nO modelo SARIMA é representado por ARIMA$(p,d,q)(P,D,Q)_s$ onde $(p,d,q)$ é a especificação do modelo ARIMA que corresponde a parte não sazonal a série e $(P,D,Q)_s$ é o modelo ARIMA que correponde a parte sazonal da série (com período $s$)."
  }
]