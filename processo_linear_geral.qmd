# Modelos ARMA

## O polinômio de defasagens

::: {#def-}
Considere uma sequência $\{a_t\}$. O operador defasagem é definido como $Ba_t = a_{t-1}$. Em inglês este operador é conhecido como *backshift*.
:::

::: {#exm-example}
Considere a série $11,6,3,7,15,8$.

$$\begin{align*}
     Ba_2 &= a_{1}=11\\
     B a_6& =a_5 =15
    \end{align*}$$ $\blacksquare$
:::

Seguem algumas propriedades fundamentais do operador defasagem

-   $Bc=c$
-   $B^k a_t=a_{t-k}$.
-   $B(c+ba_{t-1})=c+bBa_t$ (operador linear)
-   $(B^m - B^n)a_t = a_{t-m}-a_{t-n}.$
-   $B^{-1}a_t = a_{t+1}$

Note que o operador $B^{-1}$ leva $a_t$ para um passo a frente (como em uma previsão). É comum encontrar a definição $F=B^{-1}$, onde a letra $F$ é escolhida por causa do termo \textit{forecast} (previsão).

::: {#exm-}
Seja $B$ o operado defasagem. Então, definimos um polinômio de defasagens como $$\psi(B)=a_0+a_1B+\cdots+a_n B^n.$$
:::

Note que $\psi(B)$ é um modo sucinto para escrever $$a(B)x_t=a_0x_t+a_1x_{t-1}+\cdots+a_n x_{t-n}.$$

O polinômio de defasagens pode ser operado como um polinômio regular. Por exemplo, se $a(B)=1-aB$ e $b(B)=1-bB$, fazer $$\begin{align}
a(B)b(B)x_t&=a(B)[(1-bB)x_t]=a(B)(x_t-bx_{t-1})\\
&=a(B)x_t-ba(B)x_{t-1}=(1-aB)x_t-b(1-aB)x_{t-1}\\
&=x_t-(a+b)x_{t-1}+abx_{t-2}\end{align}$$ é equivalente a encontrar $$c(B)=(1-aB)(1-bB)=1-(a+b)B + abB^2$$ e calcular $$c(B)x_t=x_t-(a+b)x_{t-1}+abx_{t-2}.$$

Dizemos que o polinômio de defasagens $\psi(B)$ possui inversa se existe uma função $\psi^{-1}(B)$ tal que $\psi(B)\psi^{-1}(B)x_t=x_t$.

::: {#exm-}
Para entender corretamente os modelos que serão propostos, é fundamental entender o que é a inversa do polinômio de defasagens. O caso mais importante, que será a chave para os demais, é o baseado no seguinte polinômio: $$\psi(B)=1-\phi B.$$

Suponha que $$y_t=(1-\phi B)x_t=x_t-\phi x_{t-1 }$$ Estamos procurando que qual situação existe $\psi^{-1}(B)$ tal que $$x_t=\psi^{-1}(B)y_t.$$ Como $$y_{t-1}=(1-\phi B)x_{t-1}=x_{t-1}-\phi x_{t-2},$$ teremos que $$y_t=x_t-\phi[y_{t-1}-\phi x_{t-2}]=x_t-\phi y_{t-1}-\phi^2x_{t-2}.$$ Note que podemos continuar iterando as equações acima, obtendo $$y_t=x_t-\sum_{j=1}^{t-1} \phi^jy_{t-j}-\phi^{t}x_0.$$ Assuma que $|\phi|<1$ e que $t$ é grande, o que implica que $\phi^t x_0$ é despresível. Então $$y_t=x_t-\sum_{j=1}^{t-1}\phi^j y_{t-j}=x_t-\sum_{j=1}^{t-1}\phi^j B^j y_t$$ ou ainda $$\left(1-\sum_{j=1}^{t-1}\phi^j B^j\right)y_t=x_t$$ Agora, multiplique os dois lados da equação acima por $1-\phi B$. Então $$(1-\phi B)\left(1-\sum_{j=1}^{t-1}\phi^j B^j\right)y_t=(1-\phi B)x_t=y_t $$ Deste modo, $$\psi^{-1}(B)=\lim_{t\rightarrow \infty }\left(1-\sum_{j=1}^{t}\phi^j B^j\right)=1-\sum_{j=1}^{\infty}\phi^j B^j$$

$\blacksquare$
:::

No exemplo anterior, o fato de $|\phi|<1$ garante que a inveresa do polinômio de defasagens existem uma vez que $\lim_{t\rightarrow\infty}\sum_{j=1}^t \phi^jB^j$ é convergente.

O resultado geral é baseado no seguinte teorema.

::: {#prp-pgMatrix}
Seja $T$ uma matriz quadrada qualquer e seja $$S_n=\sum_{j=1}^n T^j.$$ A série $S_n$ converge quando $n\rightarrow\infty$ se e somente se todos os autovalores de $T$ são menores que um em módulo. Nesse caso, $T^j\rightarrow \textbf{0}$ quando $j\rightarrow\infty$ e $$S_n\rightarrow (1-T)^{-1}.$$
:::

A proposição acima é a chave para demonstrar o seguite teorema

::: {#thm-}
Seja $$y_t=x_t-\sum_{j=1}^p\phi_jx_{t-j}=\left(1-\sum_{j=1}^p\phi_jB^j\right)x_t=\phi(B).$$ Então, existe $\phi^{-1}(B)$ se e somente se o módulo das raízes de $\phi(B)$ são maiores que um.
:::

::: proof
Faremos a demonstração para o caso $p=2$, mas os mesmos passos podem ser seguidos para demonstrar o caso geral.

Seja $$y_t=x_t-\phi_1x_{t-1}-\phi_2 x_{t-2}=(1-\phi_1B-\phi_2 B^2)x_t.$$ Comecemos notando que

$$\left(\begin{array}{c} x_t \\ x_{t-1}\end{array}\right)=\left(\begin{array}{c} y_t \\ 0\end{array}\right)+\left(\begin{array}{cc} \phi_1 & \phi_2 \\ 1 & 0\end{array}\right)\left(\begin{array}{c} x_{t-1} \\ x_{t-2}\end{array}\right)$$ Fazendo $z_t=(x_t\;\;x_{t-1})$, teremos $$z_t=\left(\begin{array}{c} y_t \\ 0\end{array}\right)+\underbrace{\left(\begin{array}{cc} \phi_1 & \phi_2 \\ 1 & 0\end{array}\right)}_{A}z_{t-1}.$$ Utilizando essa relação recursiva, teremos $$z_t=A^t z_{0}+\sum_{j=0}^{t-1}A^j\left(\begin{array}{c} y_{t-j} \\ 0\end{array}\right)$$ e, notando que $x_t=(1\;\;0)z_t$, teremos $$x_t=(1\;\;0)A^t z_{0}+(1\;\;0)\sum_{j=0}^{t-1}A^j\left(\begin{array}{c} y_{t-j} \\ 0\end{array}\right)$$ Suponha que os autovalores de $A$ são, em módulo, maiores que um. Então, pela @prp-pgMatrix, para $t$ suficientemente grande, $$\begin{align}x_t&=(1\;\;0)\sum_{j=1}^\infty A^j \left(\begin{array}{c} y_{t-j} \\ 0\end{array}\right)=(1\;\;0)\sum_{j=1}^\infty A^j B^j\left(\begin{array}{c} y_{t} \\ 0\end{array}\right)\\&=\sum_{j=1}^\infty (1\;\;0)A^j B^j\left(\begin{array}{c} 1 \\ 0\end{array}\right)y_{t}=\phi^{-1}(B)y_t\end{align}$$ Agora, observe que os autovalores de $A$ são obtidos através da solução de\
$$\begin{align}0=&\left|\left(\begin{array}{cc}\phi_1 & \phi_2 \\ 1 & 0\end{array}\right)-\lambda \textbf{I}\right|-\lambda(\phi_1-\lambda)-\phi_2\\&=\lambda^2-\lambda \phi_1-\phi_2\\
&=1-\frac{1}{\lambda}\phi_1-\phi_2\frac{1}{\lambda^2}\end{align}$$ Fazendo $\lambda = 1/B$, teremos que a equação acima se torna $$0=1-\frac{1}{\lambda}\phi_1-\phi_2\frac{1}{\lambda^2}\equiv 1-\phi_1 B-\phi_2B^2=\phi(B)$$ logo, se o módulo das raízes de $\phi(B)$ são maiores que um, então o módulo dos autovalores de $A$ são menores que um e, portanto, existe $\phi^{-1}(B)$.
:::

## O modelo autorregressivo

O modelo autorregressivo de ordem $p$, ou $AR(p)$, é dado por $$\begin{equation}
        y_t = \sum_{i=1}^{p}\phi_iy_{t-i} +\varepsilon_t
\end{equation}$$ onde $\{\varepsilon_t\}$ é um ruído branco, tipicamente Normal$(0,\nu)$. Nesse modelo, a contribuição da observação $y_{t-j}$ em $y_t$ é dada por $\phi_j$, que é invariante no tempo.

Utilizando o polinômio de defasagens, pode-se escrever o modelo AR($p$) como $$\begin{equation}
        \phi(B)y_t = \varepsilon_t,
\end{equation}$$ onde $\phi(B)=1-B\phi_1-\cdots \phi_p B^p$. Se o módulo das raízes desse polinômio são maiores que um, então existe $\phi^{-1}(B)$, ou seja $$y_t=\phi^{-1}(B)\varepsilon_t=\left(\sum_{j=1}^\infty \psi_jB^j\right)\varepsilon_t$$ e o processo será estacionário, com média e variância iguais à $$\begin{align}E(y_t)&=\left(\sum_{j=1}^\infty \psi_jB^j\right)E(\varepsilon_t)=0\\
Var(y_t)&=Var\left(\sum_{j=1}^\infty \psi_jB^j\varepsilon_t\right)=\nu\sum_{j=1}^\infty \psi_j^2\\
\end{align}$$ e a função de auto covariância é dada por $$\begin{align}
\gamma(h)&=Cov(y_t,y_{t-h})\\&=Cov\left( \left(\sum_{j=1}^\infty \psi_jB^j\right)\varepsilon_t,\left(\sum_{k=1}^\infty \psi_kB^k\right)\varepsilon_{t-h}\right)\\
&=\sum_{j=1}^\infty \sum_{k=1}^\infty \psi_j\psi_k Cov\left(   \varepsilon_{t-j},\varepsilon_{t-k-h}\right)\\
&=\nu\sum_{j=1}^\infty \psi_j\psi_{j+h}\end{align}$$ {#eq-covariancia}

::: {#exm-}
Considere o processo $AR(1)$ abaixo: $$\begin{align*}
    x_t(1-B\phi)&= \varepsilon_{t-1}.
\end{align*}$$

Seja $\dot{B}$ a raiz do polinômio $1-B\phi$. Temos que $$\begin{align*}
    \phi(\dot{B})=0\Rightarrow 1-\phi \dot{B} =0 \Rightarrow \dot{B} =\frac{1}{\phi}
\end{align*}$$

Logo, o processo AR(1) é estacionário se $$|\dot{B}|>1\Rightarrow \left|\frac{1}{\phi}\right|>1\Rightarrow |\phi|< 1$$ Nesse caso, já vimos que $\phi^{-1}(B)=1-\sum_{j=1}^\infty\phi^j B^j$. Então, pela equação (@eq-covariancia), identificando $\psi_j=\phi^j$,teremos que

$$\gamma(h)=\nu\sum_{j=1}^\infty \phi^j\phi^{j+h}=\nu\phi^h\frac{\phi^2}{1-\phi^2} $$ Assim, a função de autocorrelação é dada por $$\rho(h)=\frac{\gamma(h)}{\gamma(0)}=\phi^h.$$

Note que:

-   Se $\phi\in(0,1)$, então $\rho(h)$ decai exponencialmente para zero

-   Se $\in(-1,0)$, então $\rho(h)$ decai exponencialmente para zero, mas alternando o sinal.

```{r echo = FALSE}
op <- par( mfrow = c(2,2)) 
plot( 0:10, .3^(0:10), type = 'h', lwd = 2, 
      xlab = 'Defasagem' , ylab = 'Autocorrelação')
text(5,.8,expression(phi==0.3))
abline( h= 0)
plot( 0:10, .8^(0:10), type = 'h', lwd = 2, 
      xlab = 'Defasagem' , ylab = 'Autocorrelação')
text(5,.8,expression(phi==0.8))
abline( h= 0)
plot( 0:10, (-.3)^(0:10), type = 'h', lwd = 2, 
      xlab = 'Defasagem' , ylab = 'Autocorrelação')
text(5,.8,expression(phi==-0.3))
abline( h= 0)
plot( 0:10, (-.8)^(0:10), type = 'h', lwd = 2, 
      xlab = 'Defasagem' , ylab = 'Autocorrelação')
text(5,.8,expression(phi==-0.8))
abline( h= 0)

```

$\blacksquare$
:::

::: {#exm-}
Considere que $x_t$ é um AR($2$), ou seja, $$x_t=\phi_1 x_{t-1}+\phi_2 x_{t-2}+\varepsilon_t=\phi(B)\varepsilon_t,$$ onde $\phi(B)=1-\phi_1B-\phi_2 B^2$. As raízes desse polinômio são $$\dot{B}=\frac{1}{2\phi_2}\left[\phi_1\pm\sqrt{\phi_1^2+4\phi_2}\right]$$ e, considerando que $|\dot{B}|>1$, o processo será estacionário se $(\phi_1,\phi_2)$ pertence ao triângulo delimitado pelos vértices (-2,-1), (0,1), (2,-1). É interessante notarmos que, se $\phi_2<0$, é possível que as raízes de $\phi(B)$ sejam um par de números complexos conjugados.

$\blacksquare$
:::

### Função de autocorrelação para o $AR(p)$

Consideremos o processo $$x_t = \sum_{j=1}^p\phi_j x_{t-j}+\varepsilon_t.$$ e suponha que ele é estacionário. Multiplicando ambos os lados por $x_{t-h}$ e aplicando a esperança, teremos $$\begin{align}
E(x_{t-h}x_t)&=\sum_{j=1}^p \phi_j E(x_{t-h}x_{
t-j})
\end{align}$$ e, como $\gamma(h)=E(x_t x_{t-h})$, teremos $$\begin{align}
\gamma(h)&=\sum_{j=1}^p \phi_j \gamma(h-j)
\end{align}$$ Dividindo ambos os lados por $\gamma(0)$, teremos $$\begin{align}
\rho(h)&=\sum_{j=1}^p \phi_j \rho(h-j)
\end{align}$$ Essa relação pode ser utilizada para encontrar a função de autocorrelação do processo sem a necessidade de encontrar a inversa de $\phi(B)$.

:::{#exm-}
Considere o processo $AR(2)$ abaixo: $$x_{t}=\phi_1y_{t-1}+\phi_2y_{t-2}+\varepsilon_t.$$ Multiplicando a equação acima por $x_{t-1}$ em ambos os lados teremos $$x_{t}x_{t-1}=\phi_1x_{t-1}^2+\phi_2x_{t-2}x_{t-1}+\varepsilon_t x_{t-1}.$$ Calculando o valor esperado, temos $$\begin{align*}
    \gamma(1)&=\phi_1E(x_{t-1}^2)+\phi_2E(x_{t-2}x_{t-1})+E(\varepsilon_t x_{t-1})\\
    &=\phi_1\gamma(0) + \phi_2\gamma(1).
    \end{align*}$$ Dividindo ambos os lados por $\gamma(0)$ teremos $$\rho(1)=\phi_1+\phi_2\rho(1),$$ logo, $$\rho(1)=\frac{\phi_1}{1-\phi_2}
$$

De modo análogo, teremos $$x_{t-2} x_{t}=x_{t-2}\left(\phi_1x_{t-1}+\phi_2x_{t-2}+\varepsilon_{t}\right).$$ Aplicando a esperança, teremos $$\begin{align*}
    \gamma(2)&=\phi_1 \gamma(1)+\phi_2\gamma(0).
    \end{align*}$$ e dividindo os dois lados por $\gamma(0)$ teremos\
$$\rho(2)=\phi_1\rho(1)+\phi_2=\frac{\phi_1^2}{1-\phi_2}+\phi_2$$ Com os valores de $\rho(1)$ e $\rho(2)$, podemos encontrar $\rho(3)$:

$$\rho(3)=\phi_1\rho(2)+\phi_2\rho(1)$$ e assim sucessivamente.

$\blacksquare$
:::

Em geral, a função de autocorrelação do processo AR($p$) pode ser escrita como

$$\rho(h)=\sum_{j=1}^p c_j\left(\frac{1}{\dot{B}_j}\right)^h$$ onde $\dot{B}$, novamente, é a raiz de $\phi(B)=1-\sum_{j=1}^p \phi_j B^j$.

O comportamento da função de autocorrelação depende das raízes de $\phi(B)$.

-   As raízes reais se comportam de acordo com o que já foi visto no $AR(1)$, decaindo exponencialmente.

-   As raízes complexas tem um comportamento de onda abafada exponencialmente.

::: {#exm-}
Vamos mostrar esse resultado para o caso $p=2$ (o caso geral é análogo). Teremos que

$$\begin{align}
\left(\begin{array}{c}\rho(h)\\ \rho(h-1)\end{array}\right)=\underbrace{\left(\begin{array}{cc}\phi_1 & \phi_2 \\ 1 & 0 \end{array}\right)}_{T}\left(\begin{array}{c}\rho(h-1)\\ \rho(h-2)\end{array}\right)=\cdots T^h\left(\begin{array}{c}\rho(0)\\ \rho(-1)\end{array}\right)\end{align}$$

Já discutimos, na primeira seção, que os autovalores de $T$ são equivalentes aos recíprocos das raizes de $\phi(B)$. Agora, considere a decomposição de $T$ na forma canônica $$T=Q\Lambda Q^{-1},$$ onde $Q$ é a matriz cujas colunas são formadas pelos autovetores de $T$ e $\Lambda$ é uma matriz diagonal formada pelos autovalores. Note que

$$A^2=Q\Lambda Q^{-1}Q\Lambda Q^{-1}=Q\Lambda^2 Q^{-1}$$ e, de modo análogo, $$A^h=Q\Lambda Q^{-1}Q\Lambda Q^{-1}=Q\Lambda^h Q^{-1}$$ Como $\Lambda^h$ é a matriz diagonal com os autovalores elevados à potência $h$, os elementos de $A^h$ devem ser necessariamente do tipo $\sum_{j=1}^2 c_j \lambda_j^h$.

O polinômio característico da matriz $T$ é $$\lambda^2-\phi_1\lambda-\phi_2$$ e, denominando suas raizes por $\lambda_1$ e $\lambda_2$, teremos $$\rho(h)=c_1\lambda_1^h+c_2\lambda_2^h.$$ - Se $\lambda_1$ é real então $\lambda_2$ também é real e o decaimento de $\rho(h)$ será exponencial, podendo alternar o sinal se algum dos autovalores for negativo.

-   Se $\lambda_1$ é complexo, então $\lambda_2$ será seu conjugado. Em coordenadas polares, teremos $\lambda_1=re^{i\omega}$ e $\lambda_2=re^{-i\omega}$. Deste modo, $$\begin{align}\rho(h)&=r^h [c_1e^{ih\omega}+c_2e^{-ih\omega}]\\&=r^h[(c_1+c_2)\cos(h\omega)+i(c_1-c_2)\sin(h\omega)]\end{align}$$ logo, $\rho(h)$ terá um comportamento de onda com a amplitude decrescendo exponencialmente. A próxima figura mostra esse efeito.

```{r echo = F}
set.seed(1)
x = c(0,0)
for(i in 3:100) x[i] =x[i-1] -.8*x[i-2]+rnorm(1,0,.1)
acf(x,ylab = 'Aucorrelação',xlab='Defasagens', lwd = 2, main = '')
text(10,.8,expression(phi[1]==1~','~phi[2]==-.8))
```

$\blacksquare$
:::

### A função de autocorrelação parcial (PACF)

Consideremos novamente o processo $AR(1)$. Vimos que
    $$\rho(h)=\phi^h.$$
Isto implica que $Cor(x_{t+2},x_t)=\phi^2$. Note que esta correlação só existe por causa da relação entre os pares $(x_t,x_{t+1})$ e $(x_{t+1},x_{t+2})$. De fato
    $$Cov(x_t,x_{t+2}|x_{t+1})=0.$$         

A função de autocorrelação parcial para qualquer processo estacionário depende apenas da defasagem e é definada por
$$\phi_{h}(h)=\frac{Cov\left(x_{t+h},x_t|x_{t+1},\ldots x_{t+h-1}\right)}{\sqrt{Var\left(x_t|x_{t+1},\ldots x_{t+h-1}\right)Var\left(x_{t+h}|x_{t+1},\ldots x_{t+h-1}\right)}}$$

Intuitivamente, a função de autocorrelação parcial calcula a correlação entre $x_t$ e $x_{t+h}$ eliminando a dependência linear entre os valores intermediários $x_s$, $t<s<t+h$.
  
:::{#thm-}
Seja $x_1,\ldots,x_n$ uma amostra de um processo estacionário. Sejam $\hat{\phi}_1(h),\ldots,\hat{\phi}_h(h)$ os valores que minimizam 
$$\sum_{t=h+1}^n(x_t-\phi_1x_{t-1}-\cdots-\phi_hx_{t-h})^2.$$
Então, $\hat{\phi}_h(h)$ é um estimador para a função de autocorrelação parcial
:::

Para um $AR(p)$, sempre é verdade que$\phi_{h}(h)=0$ para $h>p$. Deste modo, podemos estimar a ordem do modelo verificando a partir de qual defasagem $h$ os valores $\hat{\phi}_h(h)$ deixam serem significativos.

O gráfico da função de autocorrelação parcial constrói um gráfico com os pares $(h,\hat{\phi}_{h}(h))$ de modo semelhante ao que foi feito com o correlograma, inclusive com os mesmos intervalos para testar se a autocorrelação parcial é nula - note que a primeira defasagem é um.

No exemplo abaixo, uma amostra de tamanho 100 foi gerada de um processo AR(2) estacionário e o gráfico da função de autocorrelação parcial gerado. Note que há apenas duas autocorrelações significativas, levantando evidências de que a ordem do modelo autoregressivo é 2.

```{r}
set.seed(1)
x = c(0,0)
for(i in 3:100) x[i] = x[i-1] -.8*x[i-2]+rnorm(1,0,.1)
 
pacf(x, xlab = 'Defasagem', ylab = 'Autocorrelação parcial', main = '', lwd = 2)
```

### Método de estimação de Yule-Walker

O método de estimação de Yule-Walker para estimar os parâmetros do modelo AR($p$) consiste na aplicação do método dos momentos. Sua principal vantagem é a ausência de suposição sobre a distribuição dos erros do modelo, exigindo apenas a condição de estacionaridade.

Para o processo $AR(p)$ estacionário, sabemos que
$$\gamma(h) = \sum_{j=1}^p \phi_j \gamma( h-j).$$
Considere a equações do modelo AR($p$):
$$x_t = \sum_{j=1}^p \phi_j x_{t-j}+\varepsilon_t.$$
Podemos multiplicar ambos os lados por $x_t$ (ou seja $x_{t-h}$, com $h=0$). Após aplicar a esperança
teremos
$$E(x_t^2) = \sum_{j=1}^p \phi_j E(x_{t-j}x_{t})+E(\varepsilon_tx_{t}).$$
Aqui há um detalhe importante:  
$$E(\varepsilon_tx_{t})=E\left(\varepsilon_t\left[\sum_{j=1}^p \phi_j x_{t-j}+\varepsilon_t\right]\right)=E(\varepsilon_t^2)=\nu$$
Portanto,
$$\begin{align}   
    \gamma(h)=\left\{\begin{array}{ll}
    \sum_{j=1}^p \phi_j \gamma( h-j),&\;\;\hbox{se }h>0\\
    \sum_{j=1}^p \phi_j \gamma( j)+\nu,&\;\;\hbox{se }h=0 \\
    \end{array}\right.
    \end{align}$$

O método de estimação de Yule-Walker consiste em substituir $\gamma(1),\ldots,\gamma(p)$ pelas autocovariâncias amostrais:
$$\begin{align*}
\hat{\gamma}(1) &= \phi_1\hat{\gamma}(0) + \phi_1\hat{\gamma}(-1) + \cdots + \phi_p\hat{\gamma}(1-p) \\ 
\hat{\gamma}(2) &= \phi_1\hat{\gamma}(1) + \phi_1\hat{\gamma}(0)  + \cdots + \phi_p\hat{\gamma}(2-p) \\ 
\vdots &= \vdots \\
\hat{\gamma}(p) &= \phi_1\hat{\gamma}(p-1) + \phi_1\hat{\gamma}(p-2)  + \cdots + \phi_p\hat{\gamma}(0)
\end{align*}$$ 
Nota: lembre-se que $\gamma(-i)=\gamma(i)$. Podemos escrever o sistema na forma matricial:
$$\begin{align*}
    \underbrace{\left(
    \begin{array}{cccc}
\hat{\gamma}(0) & \hat{\gamma}(-1) & \cdots & \hat{\gamma}(1-p) \\ 
\hat{\gamma}(1) & \hat{\gamma}(0)  & \cdots & \hat{\gamma}(2-p) \\ 
         \vdots & \vdots               & \ddots                & \vdots\\
\hat{\gamma}(p-1) & \hat{\gamma}(p-2) &  \cdots & \hat{\gamma}(0)   
    \end{array}\right)}_{\hat{\boldsymbol{\Gamma}}}
    \underbrace{\left( \begin{array}{c} \hat{\phi}_1 \\ \hat{\phi}_2 \\ \vdots \\ \hat{\phi}_p   \end{array}\right)}_{\hat{\boldsymbol{\phi}}}=
    \underbrace{\left( \begin{array}{c} \hat{\gamma}(1) \\ \hat{\gamma}(2) \\ \vdots \\ \hat{\gamma}(p)   \end{array}\right)}_{\hat{\boldsymbol{\gamma}}}.
    \end{align*}$$
e os estimadores de Yule-Walker para os parâmetros autoregressivos são dados por
    $$\hat{\boldsymbol{\phi}}=\hat{\boldsymbol{\Gamma}}^{-1}\hat{\boldsymbol{\gamma}}.$$ 

Voltando para a Equação (\ref{eq:autocovarianca-AR}), temos que
$$\gamma(0)=\sum_{j=1}^{p}\phi_j \gamma(j)+\nu$$
Portanto, um estimador para $\nu$ é dado por
$$\hat{\nu}=  
  \hat{\gamma(0)}=\sum_{j=1}^{p}\hat{\phi_j}\hat{\gamma(j)}$$

### Estimador de máxima verossimilhança

Observe que
$$x_t|\mathcal{D}_{t-1}\sim x_t|x_{t-1},\ldots,x_{t-p}$$
ou seja, $\{x_t\}$ é uma cadeia de Markov de ordem $p$. Deste modo, para $n>p$,
$$\begin{align*}
f(x_{1},\ldots,x_n)&=f\left(x_n|\mathcal{D}_{n-1}\right)f\left(\mathcal{D}_{n-1}\right)=f\left(x_T|x_{n-1},\ldots,x_{n-p}\right)f\left(\mathcal{D}_{n-1}\right)\\
        &=\prod_{t=p+1}^{n}f\left(x_t|x_{t-1},\ldots,x_{t-p}\right) f(x_{1},\ldots,x_p).
        \end{align*}$$
Agora, suponha que $\varepsilon_t\sim\hbox{Normal}(0,\nu)$. Então,  $$\begin{align*}
L(\phi_1,\ldots,\phi_p,\nu)&=\left(\frac{1}{2\pi\nu}\right)^{\frac{n}{2}}\exp\left\{-\frac{1}{2\nu}\sum_{t=p+1}^n(x_t-\sum_{j=1}^p \phi_jx_{t-j})^2\right\}\\&\times f(x_{1},\ldots,x_p|\phi_1,\ldots,\phi_p,\nu).
        \end{align*}$$      


O método da maxima verossimilhança condicional consiste em ignorar o termo $f(x_{1},\ldots,x_p|\phi_1,\ldots,\phi_p,\nu)$. Com isso, o restante da função de verossimilhança pode ser escrito como um modelo linear, uma vez que

$$x_t=\sum_{j=1}^p \phi_jx_{t-j}+\varepsilon_t=\underbrace{(x_{t-1}\;\;\cdots \;\;x_{t-p})}_{F_t'}\underbrace{\left(\begin{array}{c}\phi_1 \\ \vdots \\ \phi_p\end{array}\right)}_{\beta}$$
e $\hat{\beta}$ e $\hat{\nu}$ podem ser obtidos pelos estimadores já discutidos na seção sobre modelos lineares.


Já para o método da máxima verossimilhança, precisamos especificar a distribuição do termo adicional $f(x_1,\ldots,x_p|\phi_1,\ldots,\phi_p)$. Desde que o processo seja estacionário, é possível mostrar que
$$x_1,\ldots,x_p\sim\hbox{Normal}(\textbf{0}_p,\Sigma(\phi_1,\ldots,\phi_p))$$
onde $\Sigma(\phi_1,\ldots,\phi_p))$ é a matriz formada pelas autocovariâncias.

A maximização da função de verossimilhança é feita numericamente e pode-se utilizar as estimativas obtidas no método da máxima verossimilhança condicional como ponto de partida para o algoritmo de maximização.

### Previsão

Considere a amostra $\mathcal{D}_t$ do modelo AR(1). Então,

$$\begin{align}x_{t+h}&=\phi x_{t+h-1}+\varepsilon_{t+h}\\ &=\phi^2x_{t+h-2}+\phi\varepsilon_{t+h-1}+\varepsilon_{t+h}= \cdots\\&=\phi^h x_t+\sum_{j=0}^{h-1}\phi^j\varepsilon_{t+h-j}\end{align}$$
logo
$$E(x_{t+h}|\mathcal{D}_t)=\phi^h x_t$$
e
$$Var(x_{t+h}|\mathcal{D}_t)=\nu\sum_{j=0}^{h-1}\phi^{2j}=\nu\frac{1-\phi^{2h}}{1-\phi^2}$$
Note que, se o processo é estacionário, então $|\phi|<1$ e a previsão converge para zero exponencialmente. Na prática, tanto a previsão quanto a variância são calculadas substituíndo $\phi$ por sua estimativa.

Para o processo AR$(p)$, seja $z_t=(x_t,\ldots,x_{t-p+1})$. Então,
$$z_t=Tz_{t-1}+\boldsymbol{\varepsilon}_t$$
onde
$$T=\left(\begin{array}{ccc|c}\phi_1&\cdots&\phi_{p-1} & \phi_p \\ \hline & \textbf{I}_{p-1} & & \textbf{0}_{p-1}\end{array}\right)$$
e 
$$\boldsymbol{\varepsilon}_t=\left(\begin{array}{c}\varepsilon_t \\ \hline \textbf{0}_{p-1}\end{array}\right)$$
Logo, dado $\mathcal{D}_t$,
$$z_{t+h}=T^h z_t+\sum_{j=0}^{h-1}T^j\boldsymbol{\varepsilon}_{t+h-j}$$
Note que $x_{t+h}=(1|\textbf{0}'_{p-1})z_{t+h}$, logo
$$E(x_{t+h}|\mathcal{D}_t)=(1|\textbf{0}'_{p-1})T^h z_t$$
e
$$Var(x_{t+h}|\mathcal{D}_t)=\nu\sum_{j=0}^{h-1}\left(\begin{array}{c|c}1&\textbf{0}'_{p-1}\end{array}\right)T^j \mathcal{E} T'^j\left(\begin{array}{c}1 \\ \hline \textbf{0}_{p-1}\end{array}\right) $$
onde $\mathcal{E}$ é uma matriz com o valor 1 no elemento $a_{11}$ e zero nos demais.

### Processo autorregressivo com deriva

Até o momento, assumimos que o processo AR$(p)$ é estacionário com $E(x_t)=0$. Quando $\mu=E(x_t)\neq 0$, temos um parâmetro adicional, denominado deriva e escrito como

$$x_t-\mu=\sum_{j=1}^p \phi_j(x_{t-j}-\mu)+\varepsilon_t$$

Observe que a deriva não altera a estrutura de autocorrelação do processo, uma vez que podemos definir $y_t=x_t-\mu$, onde $y_t$ é um AR($p$) com $E(y_t)=0$, o que implica em
$$\gamma(h)=Cov(x_t, x_{t-1})=Cov(x_t-\mu,x_{t-h}-\mu)=Cov(y_t,y_{t-h}).$$

Contudo, a função de previsão do processo deve ser ajusta. Defina $z_t=(x_t-\mu,\ldots,x_{t-p+1}-\mu)$. Já vimos que

$$z_{t+h}=T^h z_t+\sum_{j=0}^{h-1}T^j\boldsymbol{\varepsilon}_{t+h-j}$$
onde $T$ e $\boldsymbol{\varepsilon}_t$ foram definidos na seção anterior. Note que $x_{t+h}-\mu=(1|\textbf{0}'_{p-1})z_{t+h}$, logo
$$E(x_{t+h}|\mathcal{D}_t)=\mu+(1|\textbf{0}'_{p-1})T^h z_t$$

Em termos de estimação, a função de verossimilhança deve ser alterada para

$$\begin{align*}
L(\mu,\phi_1,\ldots,\phi_p,\nu)&=\left(\frac{1}{2\pi\nu}\right)^{\frac{n}{2}}\exp\left\{-\frac{1}{2\nu}\sum_{t=p+1}^n(x_t-\mu-\sum_{j=1}^p \phi_j(x_{t-j}-\mu)^2\right\} \\&\times f(x_{1},\ldots,x_p|\mu,\phi_1,\ldots,\phi_p,\nu).
        \end{align*}$$      
onde
$$x_{1},\ldots,x_p\sim\hbox{Normal}(\mu\textbf{1}_p,\Sigma(\phi_1,\ldots,\phi_h))$$

### Exemplo: número anual de terremotos

A série abaixo apresenta o número anual de terremotos de magnitude maior ou igual à 7 na escala Richter.


```{r}
require(gsheet)
url <- 'https://docs.google.com/spreadsheets/d/1PPf1nOjwh1fnr1VtFW2DKN6PY9s9TXEQTn6ZX3igCBQ/edit?usp=sharing'

dados <- gsheet2tbl(url)
terr <- unlist(dados[,2]) 
terr <- ts( terr, start = 1900)

ts.plot(terr, ylab = 'No terremotos', xlab = 'Ano', lwd= 2)
```
Note que o processo parece ser estacionário. Vamos explorar as funções de autocorrelação nas duas figuras abaixo. O correlograma mostra um comportamento tendendo para zero após 5 defasagens enquanto que o gráfico da função de autocorrelação parcial apresenta um único valor significativo. Temos evidências o modelo AR(1) pode ser adequado. 


```{r}
acf(terr)
pacf(terr)
```


A função `arima(x, order = c(p,0,0))` estima os parâmetros do modelo $AR(p)$ já considerando a deriva (caso contrário, utilize o argumento `include.mean=FALSE`). Abaixo apresentamos o modelo ajustado.

```{r}
mod <- arima( terr, c(1,0,0))
mod
```

Podemos verificar se o ajuste é adequado analisando os resíduos. Note que agora possuímos uma ferramente nova, o gráfico da função de autocorrelação parcial. Abaixo, os gráficos das funções de autocorrelação não apresentam valores significativos. O teste de Box-Pierce não rejeita a hipótese de um processo estacionário e o teste de Shapiro-Wilks não rejeita a hipótese de ruído branco gaussiano. Portano, temos evidências de que o modelo AR(1) com erros gaussianos é adequado para essa série.

```{r}
acf(mod$residuals)
pacf(mod$residuals)
Box.test(mod$residuals)
shapiro.test(mod$residuals)
```


As estimativas encontradas foram $\hat{\phi}=0,5$,  $\hat{\nu}=36,7$ e $\hat{\mu}=19$. Podemos utilizar a função `forecast` do pacote de mesmo nome para fazer previsões. Note que a medida que o horizonte de previsão cresce, o modelo converge para a deriva.

```{r}
require(forecast)
prev <- forecast(mod, 10)
plot(prev)
```
